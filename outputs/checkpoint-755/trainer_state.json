{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 755,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0013245033112582781,
      "grad_norm": 0.7570586800575256,
      "learning_rate": 0.0,
      "loss": 2.1428,
      "step": 1
    },
    {
      "epoch": 0.0026490066225165563,
      "grad_norm": 0.7935627102851868,
      "learning_rate": 4e-05,
      "loss": 2.1547,
      "step": 2
    },
    {
      "epoch": 0.003973509933774834,
      "grad_norm": 0.7436873912811279,
      "learning_rate": 8e-05,
      "loss": 2.085,
      "step": 3
    },
    {
      "epoch": 0.005298013245033113,
      "grad_norm": 0.8936763405799866,
      "learning_rate": 0.00012,
      "loss": 2.0432,
      "step": 4
    },
    {
      "epoch": 0.006622516556291391,
      "grad_norm": 0.9313241243362427,
      "learning_rate": 0.00016,
      "loss": 1.8476,
      "step": 5
    },
    {
      "epoch": 0.007947019867549669,
      "grad_norm": 0.9482674598693848,
      "learning_rate": 0.0002,
      "loss": 1.6073,
      "step": 6
    },
    {
      "epoch": 0.009271523178807948,
      "grad_norm": 1.0780997276306152,
      "learning_rate": 0.00019973333333333335,
      "loss": 1.2666,
      "step": 7
    },
    {
      "epoch": 0.010596026490066225,
      "grad_norm": 2.872032403945923,
      "learning_rate": 0.00019946666666666667,
      "loss": 0.9964,
      "step": 8
    },
    {
      "epoch": 0.011920529801324504,
      "grad_norm": 1.4180384874343872,
      "learning_rate": 0.00019920000000000002,
      "loss": 0.7604,
      "step": 9
    },
    {
      "epoch": 0.013245033112582781,
      "grad_norm": 1.09934401512146,
      "learning_rate": 0.00019893333333333336,
      "loss": 0.6195,
      "step": 10
    },
    {
      "epoch": 0.01456953642384106,
      "grad_norm": 0.7238682508468628,
      "learning_rate": 0.00019866666666666668,
      "loss": 0.5661,
      "step": 11
    },
    {
      "epoch": 0.015894039735099338,
      "grad_norm": 0.6455206274986267,
      "learning_rate": 0.0001984,
      "loss": 0.5493,
      "step": 12
    },
    {
      "epoch": 0.017218543046357615,
      "grad_norm": 0.5865993499755859,
      "learning_rate": 0.00019813333333333334,
      "loss": 0.4761,
      "step": 13
    },
    {
      "epoch": 0.018543046357615896,
      "grad_norm": 0.5973702073097229,
      "learning_rate": 0.00019786666666666666,
      "loss": 0.5152,
      "step": 14
    },
    {
      "epoch": 0.019867549668874173,
      "grad_norm": 0.6183328032493591,
      "learning_rate": 0.0001976,
      "loss": 0.4798,
      "step": 15
    },
    {
      "epoch": 0.02119205298013245,
      "grad_norm": 0.6679712533950806,
      "learning_rate": 0.00019733333333333335,
      "loss": 0.5054,
      "step": 16
    },
    {
      "epoch": 0.022516556291390728,
      "grad_norm": 0.6545781493186951,
      "learning_rate": 0.00019706666666666667,
      "loss": 0.5102,
      "step": 17
    },
    {
      "epoch": 0.02384105960264901,
      "grad_norm": 0.6173642873764038,
      "learning_rate": 0.0001968,
      "loss": 0.5051,
      "step": 18
    },
    {
      "epoch": 0.025165562913907286,
      "grad_norm": 0.6816730499267578,
      "learning_rate": 0.00019653333333333336,
      "loss": 0.5289,
      "step": 19
    },
    {
      "epoch": 0.026490066225165563,
      "grad_norm": 0.6664373874664307,
      "learning_rate": 0.00019626666666666668,
      "loss": 0.4828,
      "step": 20
    },
    {
      "epoch": 0.02781456953642384,
      "grad_norm": 0.6990156173706055,
      "learning_rate": 0.000196,
      "loss": 0.5232,
      "step": 21
    },
    {
      "epoch": 0.02913907284768212,
      "grad_norm": 0.7616205215454102,
      "learning_rate": 0.00019573333333333334,
      "loss": 0.4931,
      "step": 22
    },
    {
      "epoch": 0.030463576158940398,
      "grad_norm": 0.6430532336235046,
      "learning_rate": 0.00019546666666666668,
      "loss": 0.4758,
      "step": 23
    },
    {
      "epoch": 0.031788079470198675,
      "grad_norm": 0.7719194889068604,
      "learning_rate": 0.0001952,
      "loss": 0.4698,
      "step": 24
    },
    {
      "epoch": 0.033112582781456956,
      "grad_norm": 1.6802716255187988,
      "learning_rate": 0.00019493333333333335,
      "loss": 0.4315,
      "step": 25
    },
    {
      "epoch": 0.03443708609271523,
      "grad_norm": 0.582615852355957,
      "learning_rate": 0.0001946666666666667,
      "loss": 0.4368,
      "step": 26
    },
    {
      "epoch": 0.03576158940397351,
      "grad_norm": 9.4935302734375,
      "learning_rate": 0.0001944,
      "loss": 0.4678,
      "step": 27
    },
    {
      "epoch": 0.03708609271523179,
      "grad_norm": 0.6216185092926025,
      "learning_rate": 0.00019413333333333335,
      "loss": 0.4434,
      "step": 28
    },
    {
      "epoch": 0.038410596026490065,
      "grad_norm": 0.557111918926239,
      "learning_rate": 0.0001938666666666667,
      "loss": 0.4647,
      "step": 29
    },
    {
      "epoch": 0.039735099337748346,
      "grad_norm": 0.5764416456222534,
      "learning_rate": 0.00019360000000000002,
      "loss": 0.4492,
      "step": 30
    },
    {
      "epoch": 0.04105960264900662,
      "grad_norm": 0.4884883463382721,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.4384,
      "step": 31
    },
    {
      "epoch": 0.0423841059602649,
      "grad_norm": 0.41781193017959595,
      "learning_rate": 0.00019306666666666668,
      "loss": 0.4639,
      "step": 32
    },
    {
      "epoch": 0.04370860927152318,
      "grad_norm": 0.4350363612174988,
      "learning_rate": 0.0001928,
      "loss": 0.4412,
      "step": 33
    },
    {
      "epoch": 0.045033112582781455,
      "grad_norm": 0.37262916564941406,
      "learning_rate": 0.00019253333333333334,
      "loss": 0.4371,
      "step": 34
    },
    {
      "epoch": 0.046357615894039736,
      "grad_norm": 0.4092104732990265,
      "learning_rate": 0.0001922666666666667,
      "loss": 0.4507,
      "step": 35
    },
    {
      "epoch": 0.04768211920529802,
      "grad_norm": 0.5784127116203308,
      "learning_rate": 0.000192,
      "loss": 0.4197,
      "step": 36
    },
    {
      "epoch": 0.04900662251655629,
      "grad_norm": 0.35738077759742737,
      "learning_rate": 0.00019173333333333335,
      "loss": 0.3915,
      "step": 37
    },
    {
      "epoch": 0.05033112582781457,
      "grad_norm": 0.42071181535720825,
      "learning_rate": 0.0001914666666666667,
      "loss": 0.4159,
      "step": 38
    },
    {
      "epoch": 0.051655629139072845,
      "grad_norm": 0.4433196187019348,
      "learning_rate": 0.0001912,
      "loss": 0.4801,
      "step": 39
    },
    {
      "epoch": 0.052980132450331126,
      "grad_norm": 0.48367470502853394,
      "learning_rate": 0.00019093333333333333,
      "loss": 0.469,
      "step": 40
    },
    {
      "epoch": 0.054304635761589407,
      "grad_norm": 0.3625219464302063,
      "learning_rate": 0.00019066666666666668,
      "loss": 0.4142,
      "step": 41
    },
    {
      "epoch": 0.05562913907284768,
      "grad_norm": 0.45152759552001953,
      "learning_rate": 0.0001904,
      "loss": 0.431,
      "step": 42
    },
    {
      "epoch": 0.05695364238410596,
      "grad_norm": 0.3485392928123474,
      "learning_rate": 0.00019013333333333334,
      "loss": 0.3996,
      "step": 43
    },
    {
      "epoch": 0.05827814569536424,
      "grad_norm": 0.5233692526817322,
      "learning_rate": 0.00018986666666666668,
      "loss": 0.4905,
      "step": 44
    },
    {
      "epoch": 0.059602649006622516,
      "grad_norm": 0.544086754322052,
      "learning_rate": 0.0001896,
      "loss": 0.4416,
      "step": 45
    },
    {
      "epoch": 0.060927152317880796,
      "grad_norm": 0.38792651891708374,
      "learning_rate": 0.00018933333333333335,
      "loss": 0.4139,
      "step": 46
    },
    {
      "epoch": 0.06225165562913907,
      "grad_norm": 0.39303719997406006,
      "learning_rate": 0.0001890666666666667,
      "loss": 0.4088,
      "step": 47
    },
    {
      "epoch": 0.06357615894039735,
      "grad_norm": 0.3679741621017456,
      "learning_rate": 0.0001888,
      "loss": 0.3983,
      "step": 48
    },
    {
      "epoch": 0.06490066225165562,
      "grad_norm": 0.33125901222229004,
      "learning_rate": 0.00018853333333333333,
      "loss": 0.4455,
      "step": 49
    },
    {
      "epoch": 0.06622516556291391,
      "grad_norm": 0.32027584314346313,
      "learning_rate": 0.00018826666666666667,
      "loss": 0.4117,
      "step": 50
    },
    {
      "epoch": 0.06754966887417219,
      "grad_norm": 0.4061968922615051,
      "learning_rate": 0.000188,
      "loss": 0.4155,
      "step": 51
    },
    {
      "epoch": 0.06887417218543046,
      "grad_norm": 0.4371929168701172,
      "learning_rate": 0.00018773333333333333,
      "loss": 0.4591,
      "step": 52
    },
    {
      "epoch": 0.07019867549668875,
      "grad_norm": 0.39396125078201294,
      "learning_rate": 0.00018746666666666668,
      "loss": 0.4296,
      "step": 53
    },
    {
      "epoch": 0.07152317880794702,
      "grad_norm": 0.463993102312088,
      "learning_rate": 0.00018720000000000002,
      "loss": 0.479,
      "step": 54
    },
    {
      "epoch": 0.0728476821192053,
      "grad_norm": 0.3709089457988739,
      "learning_rate": 0.00018693333333333334,
      "loss": 0.4307,
      "step": 55
    },
    {
      "epoch": 0.07417218543046358,
      "grad_norm": 0.3764742910861969,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.4276,
      "step": 56
    },
    {
      "epoch": 0.07549668874172186,
      "grad_norm": 0.38127902150154114,
      "learning_rate": 0.00018640000000000003,
      "loss": 0.4565,
      "step": 57
    },
    {
      "epoch": 0.07682119205298013,
      "grad_norm": 0.5177412033081055,
      "learning_rate": 0.00018613333333333335,
      "loss": 0.4789,
      "step": 58
    },
    {
      "epoch": 0.0781456953642384,
      "grad_norm": 0.37819933891296387,
      "learning_rate": 0.00018586666666666667,
      "loss": 0.4131,
      "step": 59
    },
    {
      "epoch": 0.07947019867549669,
      "grad_norm": 0.32970327138900757,
      "learning_rate": 0.0001856,
      "loss": 0.4517,
      "step": 60
    },
    {
      "epoch": 0.08079470198675497,
      "grad_norm": 0.3404584228992462,
      "learning_rate": 0.00018533333333333333,
      "loss": 0.4315,
      "step": 61
    },
    {
      "epoch": 0.08211920529801324,
      "grad_norm": 0.4060925543308258,
      "learning_rate": 0.00018506666666666667,
      "loss": 0.4266,
      "step": 62
    },
    {
      "epoch": 0.08344370860927153,
      "grad_norm": 0.3167370855808258,
      "learning_rate": 0.00018480000000000002,
      "loss": 0.426,
      "step": 63
    },
    {
      "epoch": 0.0847682119205298,
      "grad_norm": 0.37052106857299805,
      "learning_rate": 0.00018453333333333334,
      "loss": 0.4193,
      "step": 64
    },
    {
      "epoch": 0.08609271523178808,
      "grad_norm": 0.3364921808242798,
      "learning_rate": 0.00018426666666666668,
      "loss": 0.4205,
      "step": 65
    },
    {
      "epoch": 0.08741721854304636,
      "grad_norm": 0.2835827171802521,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.3879,
      "step": 66
    },
    {
      "epoch": 0.08874172185430464,
      "grad_norm": 0.47190046310424805,
      "learning_rate": 0.00018373333333333335,
      "loss": 0.4448,
      "step": 67
    },
    {
      "epoch": 0.09006622516556291,
      "grad_norm": 0.4826417863368988,
      "learning_rate": 0.00018346666666666666,
      "loss": 0.4091,
      "step": 68
    },
    {
      "epoch": 0.0913907284768212,
      "grad_norm": 0.4669989347457886,
      "learning_rate": 0.0001832,
      "loss": 0.4354,
      "step": 69
    },
    {
      "epoch": 0.09271523178807947,
      "grad_norm": 0.49932730197906494,
      "learning_rate": 0.00018293333333333333,
      "loss": 0.4565,
      "step": 70
    },
    {
      "epoch": 0.09403973509933775,
      "grad_norm": 0.4601137936115265,
      "learning_rate": 0.00018266666666666667,
      "loss": 0.4736,
      "step": 71
    },
    {
      "epoch": 0.09536423841059603,
      "grad_norm": 0.3106260597705841,
      "learning_rate": 0.00018240000000000002,
      "loss": 0.4085,
      "step": 72
    },
    {
      "epoch": 0.09668874172185431,
      "grad_norm": 0.3775445222854614,
      "learning_rate": 0.00018213333333333333,
      "loss": 0.4536,
      "step": 73
    },
    {
      "epoch": 0.09801324503311258,
      "grad_norm": 0.5017834901809692,
      "learning_rate": 0.00018186666666666668,
      "loss": 0.431,
      "step": 74
    },
    {
      "epoch": 0.09933774834437085,
      "grad_norm": 0.30186745524406433,
      "learning_rate": 0.00018160000000000002,
      "loss": 0.3829,
      "step": 75
    },
    {
      "epoch": 0.10066225165562914,
      "grad_norm": 0.33114373683929443,
      "learning_rate": 0.00018133333333333334,
      "loss": 0.4113,
      "step": 76
    },
    {
      "epoch": 0.10198675496688742,
      "grad_norm": 0.3851037919521332,
      "learning_rate": 0.00018106666666666669,
      "loss": 0.4328,
      "step": 77
    },
    {
      "epoch": 0.10331125827814569,
      "grad_norm": 0.30356064438819885,
      "learning_rate": 0.0001808,
      "loss": 0.4097,
      "step": 78
    },
    {
      "epoch": 0.10463576158940398,
      "grad_norm": 0.3819684684276581,
      "learning_rate": 0.00018053333333333332,
      "loss": 0.4228,
      "step": 79
    },
    {
      "epoch": 0.10596026490066225,
      "grad_norm": 0.2967894673347473,
      "learning_rate": 0.00018026666666666667,
      "loss": 0.3937,
      "step": 80
    },
    {
      "epoch": 0.10728476821192053,
      "grad_norm": 0.3124581277370453,
      "learning_rate": 0.00018,
      "loss": 0.4029,
      "step": 81
    },
    {
      "epoch": 0.10860927152317881,
      "grad_norm": 0.31407126784324646,
      "learning_rate": 0.00017973333333333333,
      "loss": 0.4134,
      "step": 82
    },
    {
      "epoch": 0.10993377483443709,
      "grad_norm": 0.38140803575515747,
      "learning_rate": 0.00017946666666666667,
      "loss": 0.4184,
      "step": 83
    },
    {
      "epoch": 0.11125827814569536,
      "grad_norm": 0.41521260142326355,
      "learning_rate": 0.00017920000000000002,
      "loss": 0.4322,
      "step": 84
    },
    {
      "epoch": 0.11258278145695365,
      "grad_norm": 0.4181607663631439,
      "learning_rate": 0.00017893333333333336,
      "loss": 0.4329,
      "step": 85
    },
    {
      "epoch": 0.11390728476821192,
      "grad_norm": 0.3614547848701477,
      "learning_rate": 0.00017866666666666668,
      "loss": 0.404,
      "step": 86
    },
    {
      "epoch": 0.1152317880794702,
      "grad_norm": 0.29593390226364136,
      "learning_rate": 0.0001784,
      "loss": 0.3762,
      "step": 87
    },
    {
      "epoch": 0.11655629139072848,
      "grad_norm": 0.29681265354156494,
      "learning_rate": 0.00017813333333333334,
      "loss": 0.3852,
      "step": 88
    },
    {
      "epoch": 0.11788079470198676,
      "grad_norm": 0.31751060485839844,
      "learning_rate": 0.00017786666666666666,
      "loss": 0.4076,
      "step": 89
    },
    {
      "epoch": 0.11920529801324503,
      "grad_norm": 0.27640292048454285,
      "learning_rate": 0.0001776,
      "loss": 0.3782,
      "step": 90
    },
    {
      "epoch": 0.1205298013245033,
      "grad_norm": 0.35790374875068665,
      "learning_rate": 0.00017733333333333335,
      "loss": 0.4471,
      "step": 91
    },
    {
      "epoch": 0.12185430463576159,
      "grad_norm": 0.3539445102214813,
      "learning_rate": 0.00017706666666666667,
      "loss": 0.4267,
      "step": 92
    },
    {
      "epoch": 0.12317880794701987,
      "grad_norm": 0.37511536478996277,
      "learning_rate": 0.00017680000000000001,
      "loss": 0.4136,
      "step": 93
    },
    {
      "epoch": 0.12450331125827814,
      "grad_norm": 0.37236130237579346,
      "learning_rate": 0.00017653333333333336,
      "loss": 0.4287,
      "step": 94
    },
    {
      "epoch": 0.12582781456953643,
      "grad_norm": 0.36001554131507874,
      "learning_rate": 0.00017626666666666668,
      "loss": 0.4275,
      "step": 95
    },
    {
      "epoch": 0.1271523178807947,
      "grad_norm": 0.3509603440761566,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.4195,
      "step": 96
    },
    {
      "epoch": 0.12847682119205298,
      "grad_norm": 0.25666895508766174,
      "learning_rate": 0.00017573333333333334,
      "loss": 0.3957,
      "step": 97
    },
    {
      "epoch": 0.12980132450331125,
      "grad_norm": 0.318467915058136,
      "learning_rate": 0.00017546666666666666,
      "loss": 0.4461,
      "step": 98
    },
    {
      "epoch": 0.13112582781456952,
      "grad_norm": 0.26421108841896057,
      "learning_rate": 0.0001752,
      "loss": 0.3777,
      "step": 99
    },
    {
      "epoch": 0.13245033112582782,
      "grad_norm": 0.3557921051979065,
      "learning_rate": 0.00017493333333333335,
      "loss": 0.3939,
      "step": 100
    },
    {
      "epoch": 0.1337748344370861,
      "grad_norm": 0.39536526799201965,
      "learning_rate": 0.00017466666666666667,
      "loss": 0.4306,
      "step": 101
    },
    {
      "epoch": 0.13509933774834437,
      "grad_norm": 0.3665313422679901,
      "learning_rate": 0.0001744,
      "loss": 0.389,
      "step": 102
    },
    {
      "epoch": 0.13642384105960265,
      "grad_norm": 0.35160592198371887,
      "learning_rate": 0.00017413333333333336,
      "loss": 0.3838,
      "step": 103
    },
    {
      "epoch": 0.13774834437086092,
      "grad_norm": 0.2818484306335449,
      "learning_rate": 0.00017386666666666667,
      "loss": 0.389,
      "step": 104
    },
    {
      "epoch": 0.1390728476821192,
      "grad_norm": 0.2996926009654999,
      "learning_rate": 0.00017360000000000002,
      "loss": 0.352,
      "step": 105
    },
    {
      "epoch": 0.1403973509933775,
      "grad_norm": 0.35578688979148865,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.4086,
      "step": 106
    },
    {
      "epoch": 0.14172185430463577,
      "grad_norm": 0.338810533285141,
      "learning_rate": 0.00017306666666666665,
      "loss": 0.3973,
      "step": 107
    },
    {
      "epoch": 0.14304635761589404,
      "grad_norm": 0.35991424322128296,
      "learning_rate": 0.0001728,
      "loss": 0.3811,
      "step": 108
    },
    {
      "epoch": 0.14437086092715232,
      "grad_norm": 0.38138359785079956,
      "learning_rate": 0.00017253333333333334,
      "loss": 0.3981,
      "step": 109
    },
    {
      "epoch": 0.1456953642384106,
      "grad_norm": 0.31502318382263184,
      "learning_rate": 0.00017226666666666666,
      "loss": 0.3969,
      "step": 110
    },
    {
      "epoch": 0.14701986754966886,
      "grad_norm": 0.3553297221660614,
      "learning_rate": 0.000172,
      "loss": 0.3841,
      "step": 111
    },
    {
      "epoch": 0.14834437086092717,
      "grad_norm": 0.3948144316673279,
      "learning_rate": 0.00017173333333333335,
      "loss": 0.4368,
      "step": 112
    },
    {
      "epoch": 0.14966887417218544,
      "grad_norm": 0.375556617975235,
      "learning_rate": 0.00017146666666666667,
      "loss": 0.427,
      "step": 113
    },
    {
      "epoch": 0.1509933774834437,
      "grad_norm": 0.3377208709716797,
      "learning_rate": 0.00017120000000000001,
      "loss": 0.4069,
      "step": 114
    },
    {
      "epoch": 0.152317880794702,
      "grad_norm": 0.34382364153862,
      "learning_rate": 0.00017093333333333333,
      "loss": 0.4295,
      "step": 115
    },
    {
      "epoch": 0.15364238410596026,
      "grad_norm": 0.2807423174381256,
      "learning_rate": 0.00017066666666666668,
      "loss": 0.3872,
      "step": 116
    },
    {
      "epoch": 0.15496688741721854,
      "grad_norm": 0.2994661033153534,
      "learning_rate": 0.0001704,
      "loss": 0.3897,
      "step": 117
    },
    {
      "epoch": 0.1562913907284768,
      "grad_norm": 0.2909056842327118,
      "learning_rate": 0.00017013333333333334,
      "loss": 0.4172,
      "step": 118
    },
    {
      "epoch": 0.1576158940397351,
      "grad_norm": 0.3350118398666382,
      "learning_rate": 0.00016986666666666668,
      "loss": 0.3714,
      "step": 119
    },
    {
      "epoch": 0.15894039735099338,
      "grad_norm": 0.2688073515892029,
      "learning_rate": 0.0001696,
      "loss": 0.3942,
      "step": 120
    },
    {
      "epoch": 0.16026490066225166,
      "grad_norm": 0.35406580567359924,
      "learning_rate": 0.00016933333333333335,
      "loss": 0.42,
      "step": 121
    },
    {
      "epoch": 0.16158940397350993,
      "grad_norm": 0.2752246558666229,
      "learning_rate": 0.0001690666666666667,
      "loss": 0.3632,
      "step": 122
    },
    {
      "epoch": 0.1629139072847682,
      "grad_norm": 0.2523481249809265,
      "learning_rate": 0.0001688,
      "loss": 0.3657,
      "step": 123
    },
    {
      "epoch": 0.16423841059602648,
      "grad_norm": 0.3176986873149872,
      "learning_rate": 0.00016853333333333336,
      "loss": 0.4074,
      "step": 124
    },
    {
      "epoch": 0.16556291390728478,
      "grad_norm": 0.3157024681568146,
      "learning_rate": 0.00016826666666666667,
      "loss": 0.4137,
      "step": 125
    },
    {
      "epoch": 0.16688741721854305,
      "grad_norm": 0.3855413496494293,
      "learning_rate": 0.000168,
      "loss": 0.4175,
      "step": 126
    },
    {
      "epoch": 0.16821192052980133,
      "grad_norm": 0.32242387533187866,
      "learning_rate": 0.00016773333333333334,
      "loss": 0.4025,
      "step": 127
    },
    {
      "epoch": 0.1695364238410596,
      "grad_norm": 0.37815433740615845,
      "learning_rate": 0.00016746666666666668,
      "loss": 0.4067,
      "step": 128
    },
    {
      "epoch": 0.17086092715231788,
      "grad_norm": 0.4949069321155548,
      "learning_rate": 0.0001672,
      "loss": 0.4078,
      "step": 129
    },
    {
      "epoch": 0.17218543046357615,
      "grad_norm": 0.2993059456348419,
      "learning_rate": 0.00016693333333333334,
      "loss": 0.3928,
      "step": 130
    },
    {
      "epoch": 0.17350993377483442,
      "grad_norm": 0.26067054271698,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.3836,
      "step": 131
    },
    {
      "epoch": 0.17483443708609273,
      "grad_norm": 0.3361421227455139,
      "learning_rate": 0.0001664,
      "loss": 0.3802,
      "step": 132
    },
    {
      "epoch": 0.176158940397351,
      "grad_norm": 0.2765447497367859,
      "learning_rate": 0.00016613333333333335,
      "loss": 0.3957,
      "step": 133
    },
    {
      "epoch": 0.17748344370860927,
      "grad_norm": 0.25559282302856445,
      "learning_rate": 0.00016586666666666667,
      "loss": 0.3763,
      "step": 134
    },
    {
      "epoch": 0.17880794701986755,
      "grad_norm": 0.3537824749946594,
      "learning_rate": 0.0001656,
      "loss": 0.4023,
      "step": 135
    },
    {
      "epoch": 0.18013245033112582,
      "grad_norm": 0.3693350553512573,
      "learning_rate": 0.00016533333333333333,
      "loss": 0.4038,
      "step": 136
    },
    {
      "epoch": 0.1814569536423841,
      "grad_norm": 0.4511041045188904,
      "learning_rate": 0.00016506666666666668,
      "loss": 0.4536,
      "step": 137
    },
    {
      "epoch": 0.1827814569536424,
      "grad_norm": 0.4572365880012512,
      "learning_rate": 0.0001648,
      "loss": 0.439,
      "step": 138
    },
    {
      "epoch": 0.18410596026490067,
      "grad_norm": 0.2585262656211853,
      "learning_rate": 0.00016453333333333334,
      "loss": 0.3898,
      "step": 139
    },
    {
      "epoch": 0.18543046357615894,
      "grad_norm": 0.3648831248283386,
      "learning_rate": 0.00016426666666666668,
      "loss": 0.4049,
      "step": 140
    },
    {
      "epoch": 0.18675496688741722,
      "grad_norm": 0.320568710565567,
      "learning_rate": 0.000164,
      "loss": 0.3988,
      "step": 141
    },
    {
      "epoch": 0.1880794701986755,
      "grad_norm": 0.2743482291698456,
      "learning_rate": 0.00016373333333333335,
      "loss": 0.3633,
      "step": 142
    },
    {
      "epoch": 0.18940397350993377,
      "grad_norm": 0.3101021349430084,
      "learning_rate": 0.0001634666666666667,
      "loss": 0.3997,
      "step": 143
    },
    {
      "epoch": 0.19072847682119207,
      "grad_norm": 0.34020426869392395,
      "learning_rate": 0.0001632,
      "loss": 0.4019,
      "step": 144
    },
    {
      "epoch": 0.19205298013245034,
      "grad_norm": 0.3685421347618103,
      "learning_rate": 0.00016293333333333333,
      "loss": 0.4072,
      "step": 145
    },
    {
      "epoch": 0.19337748344370861,
      "grad_norm": 0.2999173402786255,
      "learning_rate": 0.00016266666666666667,
      "loss": 0.3996,
      "step": 146
    },
    {
      "epoch": 0.1947019867549669,
      "grad_norm": 0.35215407609939575,
      "learning_rate": 0.00016240000000000002,
      "loss": 0.4023,
      "step": 147
    },
    {
      "epoch": 0.19602649006622516,
      "grad_norm": 0.4753045439720154,
      "learning_rate": 0.00016213333333333334,
      "loss": 0.457,
      "step": 148
    },
    {
      "epoch": 0.19735099337748344,
      "grad_norm": 0.3466431200504303,
      "learning_rate": 0.00016186666666666668,
      "loss": 0.3897,
      "step": 149
    },
    {
      "epoch": 0.1986754966887417,
      "grad_norm": 0.3019753098487854,
      "learning_rate": 0.00016160000000000002,
      "loss": 0.3925,
      "step": 150
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.31117093563079834,
      "learning_rate": 0.00016133333333333334,
      "loss": 0.4161,
      "step": 151
    },
    {
      "epoch": 0.20132450331125828,
      "grad_norm": 0.5691999197006226,
      "learning_rate": 0.0001610666666666667,
      "loss": 0.385,
      "step": 152
    },
    {
      "epoch": 0.20264900662251656,
      "grad_norm": 0.3874674141407013,
      "learning_rate": 0.0001608,
      "loss": 0.4088,
      "step": 153
    },
    {
      "epoch": 0.20397350993377483,
      "grad_norm": 0.2562366724014282,
      "learning_rate": 0.00016053333333333332,
      "loss": 0.3775,
      "step": 154
    },
    {
      "epoch": 0.2052980132450331,
      "grad_norm": 0.26187124848365784,
      "learning_rate": 0.00016026666666666667,
      "loss": 0.3922,
      "step": 155
    },
    {
      "epoch": 0.20662251655629138,
      "grad_norm": 0.3225492537021637,
      "learning_rate": 0.00016,
      "loss": 0.3745,
      "step": 156
    },
    {
      "epoch": 0.20794701986754968,
      "grad_norm": 0.5382707715034485,
      "learning_rate": 0.00015973333333333333,
      "loss": 0.4192,
      "step": 157
    },
    {
      "epoch": 0.20927152317880796,
      "grad_norm": 0.4087713956832886,
      "learning_rate": 0.00015946666666666668,
      "loss": 0.4648,
      "step": 158
    },
    {
      "epoch": 0.21059602649006623,
      "grad_norm": 0.30039042234420776,
      "learning_rate": 0.00015920000000000002,
      "loss": 0.4009,
      "step": 159
    },
    {
      "epoch": 0.2119205298013245,
      "grad_norm": 0.24106371402740479,
      "learning_rate": 0.00015893333333333334,
      "loss": 0.3888,
      "step": 160
    },
    {
      "epoch": 0.21324503311258278,
      "grad_norm": 0.28690558671951294,
      "learning_rate": 0.00015866666666666668,
      "loss": 0.3646,
      "step": 161
    },
    {
      "epoch": 0.21456953642384105,
      "grad_norm": 0.29225388169288635,
      "learning_rate": 0.00015840000000000003,
      "loss": 0.3724,
      "step": 162
    },
    {
      "epoch": 0.21589403973509932,
      "grad_norm": 0.3199094235897064,
      "learning_rate": 0.00015813333333333335,
      "loss": 0.4223,
      "step": 163
    },
    {
      "epoch": 0.21721854304635763,
      "grad_norm": 0.24403733015060425,
      "learning_rate": 0.00015786666666666666,
      "loss": 0.3535,
      "step": 164
    },
    {
      "epoch": 0.2185430463576159,
      "grad_norm": 0.32609453797340393,
      "learning_rate": 0.0001576,
      "loss": 0.413,
      "step": 165
    },
    {
      "epoch": 0.21986754966887417,
      "grad_norm": 0.24738304316997528,
      "learning_rate": 0.00015733333333333333,
      "loss": 0.3673,
      "step": 166
    },
    {
      "epoch": 0.22119205298013245,
      "grad_norm": 0.3784869611263275,
      "learning_rate": 0.00015706666666666667,
      "loss": 0.4246,
      "step": 167
    },
    {
      "epoch": 0.22251655629139072,
      "grad_norm": 0.294360488653183,
      "learning_rate": 0.00015680000000000002,
      "loss": 0.4064,
      "step": 168
    },
    {
      "epoch": 0.223841059602649,
      "grad_norm": 0.3343261182308197,
      "learning_rate": 0.00015653333333333333,
      "loss": 0.409,
      "step": 169
    },
    {
      "epoch": 0.2251655629139073,
      "grad_norm": 0.2852400839328766,
      "learning_rate": 0.00015626666666666668,
      "loss": 0.4073,
      "step": 170
    },
    {
      "epoch": 0.22649006622516557,
      "grad_norm": 0.3120308816432953,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.4149,
      "step": 171
    },
    {
      "epoch": 0.22781456953642384,
      "grad_norm": 0.4111117124557495,
      "learning_rate": 0.00015573333333333334,
      "loss": 0.4083,
      "step": 172
    },
    {
      "epoch": 0.22913907284768212,
      "grad_norm": 0.40968528389930725,
      "learning_rate": 0.00015546666666666666,
      "loss": 0.3759,
      "step": 173
    },
    {
      "epoch": 0.2304635761589404,
      "grad_norm": 0.3456040620803833,
      "learning_rate": 0.0001552,
      "loss": 0.4032,
      "step": 174
    },
    {
      "epoch": 0.23178807947019867,
      "grad_norm": 0.2995947301387787,
      "learning_rate": 0.00015493333333333332,
      "loss": 0.4219,
      "step": 175
    },
    {
      "epoch": 0.23311258278145697,
      "grad_norm": 0.392891526222229,
      "learning_rate": 0.00015466666666666667,
      "loss": 0.3984,
      "step": 176
    },
    {
      "epoch": 0.23443708609271524,
      "grad_norm": 0.26277756690979004,
      "learning_rate": 0.0001544,
      "loss": 0.3905,
      "step": 177
    },
    {
      "epoch": 0.23576158940397351,
      "grad_norm": 0.32274940609931946,
      "learning_rate": 0.00015413333333333336,
      "loss": 0.3736,
      "step": 178
    },
    {
      "epoch": 0.2370860927152318,
      "grad_norm": 0.4276736080646515,
      "learning_rate": 0.00015386666666666668,
      "loss": 0.3761,
      "step": 179
    },
    {
      "epoch": 0.23841059602649006,
      "grad_norm": 0.3181953430175781,
      "learning_rate": 0.00015360000000000002,
      "loss": 0.3974,
      "step": 180
    },
    {
      "epoch": 0.23973509933774834,
      "grad_norm": 0.37343713641166687,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.4357,
      "step": 181
    },
    {
      "epoch": 0.2410596026490066,
      "grad_norm": 0.2756667137145996,
      "learning_rate": 0.00015306666666666666,
      "loss": 0.3869,
      "step": 182
    },
    {
      "epoch": 0.2423841059602649,
      "grad_norm": 0.40226608514785767,
      "learning_rate": 0.0001528,
      "loss": 0.4223,
      "step": 183
    },
    {
      "epoch": 0.24370860927152319,
      "grad_norm": 0.2520397901535034,
      "learning_rate": 0.00015253333333333335,
      "loss": 0.3696,
      "step": 184
    },
    {
      "epoch": 0.24503311258278146,
      "grad_norm": 0.2807076871395111,
      "learning_rate": 0.00015226666666666666,
      "loss": 0.4057,
      "step": 185
    },
    {
      "epoch": 0.24635761589403973,
      "grad_norm": 0.31734558939933777,
      "learning_rate": 0.000152,
      "loss": 0.3683,
      "step": 186
    },
    {
      "epoch": 0.247682119205298,
      "grad_norm": 0.4777661859989166,
      "learning_rate": 0.00015173333333333335,
      "loss": 0.4097,
      "step": 187
    },
    {
      "epoch": 0.24900662251655628,
      "grad_norm": 0.2574882209300995,
      "learning_rate": 0.00015146666666666667,
      "loss": 0.3511,
      "step": 188
    },
    {
      "epoch": 0.2503311258278146,
      "grad_norm": 0.3202632665634155,
      "learning_rate": 0.00015120000000000002,
      "loss": 0.3896,
      "step": 189
    },
    {
      "epoch": 0.25165562913907286,
      "grad_norm": 0.2430245280265808,
      "learning_rate": 0.00015093333333333336,
      "loss": 0.4034,
      "step": 190
    },
    {
      "epoch": 0.25298013245033113,
      "grad_norm": 0.2594963312149048,
      "learning_rate": 0.00015066666666666668,
      "loss": 0.388,
      "step": 191
    },
    {
      "epoch": 0.2543046357615894,
      "grad_norm": 0.3050554692745209,
      "learning_rate": 0.0001504,
      "loss": 0.3732,
      "step": 192
    },
    {
      "epoch": 0.2556291390728477,
      "grad_norm": 0.2959580421447754,
      "learning_rate": 0.00015013333333333334,
      "loss": 0.3419,
      "step": 193
    },
    {
      "epoch": 0.25695364238410595,
      "grad_norm": 0.43640661239624023,
      "learning_rate": 0.00014986666666666666,
      "loss": 0.3913,
      "step": 194
    },
    {
      "epoch": 0.2582781456953642,
      "grad_norm": 0.45778346061706543,
      "learning_rate": 0.0001496,
      "loss": 0.3717,
      "step": 195
    },
    {
      "epoch": 0.2596026490066225,
      "grad_norm": 0.31877321004867554,
      "learning_rate": 0.00014933333333333335,
      "loss": 0.422,
      "step": 196
    },
    {
      "epoch": 0.2609271523178808,
      "grad_norm": 0.29619935154914856,
      "learning_rate": 0.00014906666666666667,
      "loss": 0.37,
      "step": 197
    },
    {
      "epoch": 0.26225165562913905,
      "grad_norm": 0.26349955797195435,
      "learning_rate": 0.0001488,
      "loss": 0.3816,
      "step": 198
    },
    {
      "epoch": 0.2635761589403974,
      "grad_norm": 0.32817959785461426,
      "learning_rate": 0.00014853333333333336,
      "loss": 0.3975,
      "step": 199
    },
    {
      "epoch": 0.26490066225165565,
      "grad_norm": 0.2709062397480011,
      "learning_rate": 0.00014826666666666667,
      "loss": 0.3888,
      "step": 200
    },
    {
      "epoch": 0.2662251655629139,
      "grad_norm": 0.35116851329803467,
      "learning_rate": 0.000148,
      "loss": 0.404,
      "step": 201
    },
    {
      "epoch": 0.2675496688741722,
      "grad_norm": 0.24751904606819153,
      "learning_rate": 0.00014773333333333334,
      "loss": 0.4145,
      "step": 202
    },
    {
      "epoch": 0.26887417218543047,
      "grad_norm": 0.28770187497138977,
      "learning_rate": 0.00014746666666666666,
      "loss": 0.3546,
      "step": 203
    },
    {
      "epoch": 0.27019867549668874,
      "grad_norm": 0.2988346517086029,
      "learning_rate": 0.0001472,
      "loss": 0.3979,
      "step": 204
    },
    {
      "epoch": 0.271523178807947,
      "grad_norm": 0.3626476526260376,
      "learning_rate": 0.00014693333333333335,
      "loss": 0.4056,
      "step": 205
    },
    {
      "epoch": 0.2728476821192053,
      "grad_norm": 0.2584153711795807,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.3898,
      "step": 206
    },
    {
      "epoch": 0.27417218543046357,
      "grad_norm": 0.37448689341545105,
      "learning_rate": 0.0001464,
      "loss": 0.3856,
      "step": 207
    },
    {
      "epoch": 0.27549668874172184,
      "grad_norm": 0.3143482506275177,
      "learning_rate": 0.00014613333333333335,
      "loss": 0.4104,
      "step": 208
    },
    {
      "epoch": 0.2768211920529801,
      "grad_norm": 0.29708683490753174,
      "learning_rate": 0.00014586666666666667,
      "loss": 0.421,
      "step": 209
    },
    {
      "epoch": 0.2781456953642384,
      "grad_norm": 0.35930338501930237,
      "learning_rate": 0.00014560000000000002,
      "loss": 0.3892,
      "step": 210
    },
    {
      "epoch": 0.27947019867549666,
      "grad_norm": 0.3607717454433441,
      "learning_rate": 0.00014533333333333333,
      "loss": 0.4,
      "step": 211
    },
    {
      "epoch": 0.280794701986755,
      "grad_norm": 0.2552420496940613,
      "learning_rate": 0.00014506666666666668,
      "loss": 0.3602,
      "step": 212
    },
    {
      "epoch": 0.28211920529801326,
      "grad_norm": 0.2520768940448761,
      "learning_rate": 0.0001448,
      "loss": 0.3799,
      "step": 213
    },
    {
      "epoch": 0.28344370860927154,
      "grad_norm": 0.26543354988098145,
      "learning_rate": 0.00014453333333333334,
      "loss": 0.3964,
      "step": 214
    },
    {
      "epoch": 0.2847682119205298,
      "grad_norm": 0.3358800411224365,
      "learning_rate": 0.00014426666666666669,
      "loss": 0.3607,
      "step": 215
    },
    {
      "epoch": 0.2860927152317881,
      "grad_norm": 0.289557546377182,
      "learning_rate": 0.000144,
      "loss": 0.3829,
      "step": 216
    },
    {
      "epoch": 0.28741721854304636,
      "grad_norm": 0.275240033864975,
      "learning_rate": 0.00014373333333333335,
      "loss": 0.3888,
      "step": 217
    },
    {
      "epoch": 0.28874172185430463,
      "grad_norm": 0.2707309126853943,
      "learning_rate": 0.0001434666666666667,
      "loss": 0.3893,
      "step": 218
    },
    {
      "epoch": 0.2900662251655629,
      "grad_norm": 0.49044331908226013,
      "learning_rate": 0.0001432,
      "loss": 0.3862,
      "step": 219
    },
    {
      "epoch": 0.2913907284768212,
      "grad_norm": 0.30748239159584045,
      "learning_rate": 0.00014293333333333333,
      "loss": 0.3978,
      "step": 220
    },
    {
      "epoch": 0.29271523178807946,
      "grad_norm": 0.27762311697006226,
      "learning_rate": 0.00014266666666666667,
      "loss": 0.3713,
      "step": 221
    },
    {
      "epoch": 0.29403973509933773,
      "grad_norm": 0.39811137318611145,
      "learning_rate": 0.0001424,
      "loss": 0.3652,
      "step": 222
    },
    {
      "epoch": 0.295364238410596,
      "grad_norm": 0.3110564351081848,
      "learning_rate": 0.00014213333333333334,
      "loss": 0.402,
      "step": 223
    },
    {
      "epoch": 0.29668874172185433,
      "grad_norm": 0.29456132650375366,
      "learning_rate": 0.00014186666666666668,
      "loss": 0.4152,
      "step": 224
    },
    {
      "epoch": 0.2980132450331126,
      "grad_norm": 0.3952343463897705,
      "learning_rate": 0.0001416,
      "loss": 0.4127,
      "step": 225
    },
    {
      "epoch": 0.2993377483443709,
      "grad_norm": 0.3437362313270569,
      "learning_rate": 0.00014133333333333334,
      "loss": 0.4028,
      "step": 226
    },
    {
      "epoch": 0.30066225165562915,
      "grad_norm": 0.30434903502464294,
      "learning_rate": 0.0001410666666666667,
      "loss": 0.3485,
      "step": 227
    },
    {
      "epoch": 0.3019867549668874,
      "grad_norm": 0.27405205368995667,
      "learning_rate": 0.0001408,
      "loss": 0.4024,
      "step": 228
    },
    {
      "epoch": 0.3033112582781457,
      "grad_norm": 0.2875944972038269,
      "learning_rate": 0.00014053333333333335,
      "loss": 0.4188,
      "step": 229
    },
    {
      "epoch": 0.304635761589404,
      "grad_norm": 0.2898653447628021,
      "learning_rate": 0.00014026666666666667,
      "loss": 0.3931,
      "step": 230
    },
    {
      "epoch": 0.30596026490066225,
      "grad_norm": 0.30834272503852844,
      "learning_rate": 0.00014,
      "loss": 0.3886,
      "step": 231
    },
    {
      "epoch": 0.3072847682119205,
      "grad_norm": 0.31996068358421326,
      "learning_rate": 0.00013973333333333333,
      "loss": 0.3851,
      "step": 232
    },
    {
      "epoch": 0.3086092715231788,
      "grad_norm": 0.26773303747177124,
      "learning_rate": 0.00013946666666666668,
      "loss": 0.3771,
      "step": 233
    },
    {
      "epoch": 0.30993377483443707,
      "grad_norm": 0.3069915473461151,
      "learning_rate": 0.0001392,
      "loss": 0.3717,
      "step": 234
    },
    {
      "epoch": 0.31125827814569534,
      "grad_norm": 0.36636069416999817,
      "learning_rate": 0.00013893333333333334,
      "loss": 0.3903,
      "step": 235
    },
    {
      "epoch": 0.3125827814569536,
      "grad_norm": 0.3201671242713928,
      "learning_rate": 0.00013866666666666669,
      "loss": 0.4026,
      "step": 236
    },
    {
      "epoch": 0.31390728476821195,
      "grad_norm": 0.28937485814094543,
      "learning_rate": 0.0001384,
      "loss": 0.4046,
      "step": 237
    },
    {
      "epoch": 0.3152317880794702,
      "grad_norm": 0.29602429270744324,
      "learning_rate": 0.00013813333333333335,
      "loss": 0.3772,
      "step": 238
    },
    {
      "epoch": 0.3165562913907285,
      "grad_norm": 0.38715285062789917,
      "learning_rate": 0.00013786666666666667,
      "loss": 0.4209,
      "step": 239
    },
    {
      "epoch": 0.31788079470198677,
      "grad_norm": 0.32656121253967285,
      "learning_rate": 0.00013759999999999998,
      "loss": 0.3749,
      "step": 240
    },
    {
      "epoch": 0.31920529801324504,
      "grad_norm": 0.4037879407405853,
      "learning_rate": 0.00013733333333333333,
      "loss": 0.378,
      "step": 241
    },
    {
      "epoch": 0.3205298013245033,
      "grad_norm": 0.2600170075893402,
      "learning_rate": 0.00013706666666666667,
      "loss": 0.3625,
      "step": 242
    },
    {
      "epoch": 0.3218543046357616,
      "grad_norm": 0.27245640754699707,
      "learning_rate": 0.00013680000000000002,
      "loss": 0.3857,
      "step": 243
    },
    {
      "epoch": 0.32317880794701986,
      "grad_norm": 0.2564908266067505,
      "learning_rate": 0.00013653333333333334,
      "loss": 0.3527,
      "step": 244
    },
    {
      "epoch": 0.32450331125827814,
      "grad_norm": 0.44501668214797974,
      "learning_rate": 0.00013626666666666668,
      "loss": 0.396,
      "step": 245
    },
    {
      "epoch": 0.3258278145695364,
      "grad_norm": 0.27280205488204956,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.3771,
      "step": 246
    },
    {
      "epoch": 0.3271523178807947,
      "grad_norm": 0.27372992038726807,
      "learning_rate": 0.00013573333333333334,
      "loss": 0.3528,
      "step": 247
    },
    {
      "epoch": 0.32847682119205296,
      "grad_norm": 0.3039800822734833,
      "learning_rate": 0.00013546666666666666,
      "loss": 0.373,
      "step": 248
    },
    {
      "epoch": 0.32980132450331123,
      "grad_norm": 0.3259023427963257,
      "learning_rate": 0.0001352,
      "loss": 0.4318,
      "step": 249
    },
    {
      "epoch": 0.33112582781456956,
      "grad_norm": 0.32028648257255554,
      "learning_rate": 0.00013493333333333332,
      "loss": 0.419,
      "step": 250
    },
    {
      "epoch": 0.33245033112582784,
      "grad_norm": 0.3548317551612854,
      "learning_rate": 0.00013466666666666667,
      "loss": 0.3879,
      "step": 251
    },
    {
      "epoch": 0.3337748344370861,
      "grad_norm": 0.32962602376937866,
      "learning_rate": 0.00013440000000000001,
      "loss": 0.3792,
      "step": 252
    },
    {
      "epoch": 0.3350993377483444,
      "grad_norm": 0.2802965044975281,
      "learning_rate": 0.00013413333333333333,
      "loss": 0.3923,
      "step": 253
    },
    {
      "epoch": 0.33642384105960266,
      "grad_norm": 0.2528424561023712,
      "learning_rate": 0.00013386666666666668,
      "loss": 0.35,
      "step": 254
    },
    {
      "epoch": 0.33774834437086093,
      "grad_norm": 0.36371099948883057,
      "learning_rate": 0.00013360000000000002,
      "loss": 0.3864,
      "step": 255
    },
    {
      "epoch": 0.3390728476821192,
      "grad_norm": 0.2761688232421875,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.3931,
      "step": 256
    },
    {
      "epoch": 0.3403973509933775,
      "grad_norm": 0.454926073551178,
      "learning_rate": 0.00013306666666666668,
      "loss": 0.3579,
      "step": 257
    },
    {
      "epoch": 0.34172185430463575,
      "grad_norm": 0.2657182216644287,
      "learning_rate": 0.0001328,
      "loss": 0.3945,
      "step": 258
    },
    {
      "epoch": 0.343046357615894,
      "grad_norm": 0.3375859260559082,
      "learning_rate": 0.00013253333333333332,
      "loss": 0.3959,
      "step": 259
    },
    {
      "epoch": 0.3443708609271523,
      "grad_norm": 0.2680225074291229,
      "learning_rate": 0.00013226666666666667,
      "loss": 0.3924,
      "step": 260
    },
    {
      "epoch": 0.3456953642384106,
      "grad_norm": 0.28358134627342224,
      "learning_rate": 0.000132,
      "loss": 0.3826,
      "step": 261
    },
    {
      "epoch": 0.34701986754966885,
      "grad_norm": 0.2831663489341736,
      "learning_rate": 0.00013173333333333333,
      "loss": 0.396,
      "step": 262
    },
    {
      "epoch": 0.3483443708609272,
      "grad_norm": 0.36324673891067505,
      "learning_rate": 0.00013146666666666667,
      "loss": 0.3478,
      "step": 263
    },
    {
      "epoch": 0.34966887417218545,
      "grad_norm": 0.2977837026119232,
      "learning_rate": 0.00013120000000000002,
      "loss": 0.3746,
      "step": 264
    },
    {
      "epoch": 0.3509933774834437,
      "grad_norm": 0.2620254158973694,
      "learning_rate": 0.00013093333333333334,
      "loss": 0.3731,
      "step": 265
    },
    {
      "epoch": 0.352317880794702,
      "grad_norm": 0.3046436309814453,
      "learning_rate": 0.00013066666666666668,
      "loss": 0.3991,
      "step": 266
    },
    {
      "epoch": 0.3536423841059603,
      "grad_norm": 0.3008890151977539,
      "learning_rate": 0.0001304,
      "loss": 0.3814,
      "step": 267
    },
    {
      "epoch": 0.35496688741721855,
      "grad_norm": 0.2705863118171692,
      "learning_rate": 0.00013013333333333332,
      "loss": 0.3506,
      "step": 268
    },
    {
      "epoch": 0.3562913907284768,
      "grad_norm": 0.3131200075149536,
      "learning_rate": 0.00012986666666666666,
      "loss": 0.3844,
      "step": 269
    },
    {
      "epoch": 0.3576158940397351,
      "grad_norm": 0.5351498126983643,
      "learning_rate": 0.0001296,
      "loss": 0.3882,
      "step": 270
    },
    {
      "epoch": 0.35894039735099337,
      "grad_norm": 0.3859928846359253,
      "learning_rate": 0.00012933333333333332,
      "loss": 0.4435,
      "step": 271
    },
    {
      "epoch": 0.36026490066225164,
      "grad_norm": 0.2694542407989502,
      "learning_rate": 0.00012906666666666667,
      "loss": 0.3583,
      "step": 272
    },
    {
      "epoch": 0.3615894039735099,
      "grad_norm": 0.2579531967639923,
      "learning_rate": 0.00012880000000000001,
      "loss": 0.3773,
      "step": 273
    },
    {
      "epoch": 0.3629139072847682,
      "grad_norm": 0.336603581905365,
      "learning_rate": 0.00012853333333333336,
      "loss": 0.3869,
      "step": 274
    },
    {
      "epoch": 0.36423841059602646,
      "grad_norm": 0.2780587673187256,
      "learning_rate": 0.00012826666666666668,
      "loss": 0.3747,
      "step": 275
    },
    {
      "epoch": 0.3655629139072848,
      "grad_norm": 0.4561136066913605,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.3932,
      "step": 276
    },
    {
      "epoch": 0.36688741721854307,
      "grad_norm": 0.28142037987709045,
      "learning_rate": 0.00012773333333333334,
      "loss": 0.3582,
      "step": 277
    },
    {
      "epoch": 0.36821192052980134,
      "grad_norm": 0.28365597128868103,
      "learning_rate": 0.00012746666666666666,
      "loss": 0.3664,
      "step": 278
    },
    {
      "epoch": 0.3695364238410596,
      "grad_norm": 0.41158100962638855,
      "learning_rate": 0.0001272,
      "loss": 0.4051,
      "step": 279
    },
    {
      "epoch": 0.3708609271523179,
      "grad_norm": 0.24202291667461395,
      "learning_rate": 0.00012693333333333335,
      "loss": 0.3686,
      "step": 280
    },
    {
      "epoch": 0.37218543046357616,
      "grad_norm": 0.2691579759120941,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.3646,
      "step": 281
    },
    {
      "epoch": 0.37350993377483444,
      "grad_norm": 0.3162081837654114,
      "learning_rate": 0.0001264,
      "loss": 0.3925,
      "step": 282
    },
    {
      "epoch": 0.3748344370860927,
      "grad_norm": 0.2813042998313904,
      "learning_rate": 0.00012613333333333335,
      "loss": 0.3735,
      "step": 283
    },
    {
      "epoch": 0.376158940397351,
      "grad_norm": 0.24063469469547272,
      "learning_rate": 0.00012586666666666667,
      "loss": 0.3563,
      "step": 284
    },
    {
      "epoch": 0.37748344370860926,
      "grad_norm": 0.28857502341270447,
      "learning_rate": 0.00012560000000000002,
      "loss": 0.3603,
      "step": 285
    },
    {
      "epoch": 0.37880794701986753,
      "grad_norm": 0.2691866159439087,
      "learning_rate": 0.00012533333333333334,
      "loss": 0.3695,
      "step": 286
    },
    {
      "epoch": 0.3801324503311258,
      "grad_norm": 0.2639743685722351,
      "learning_rate": 0.00012506666666666665,
      "loss": 0.3816,
      "step": 287
    },
    {
      "epoch": 0.38145695364238413,
      "grad_norm": 0.30834072828292847,
      "learning_rate": 0.0001248,
      "loss": 0.3623,
      "step": 288
    },
    {
      "epoch": 0.3827814569536424,
      "grad_norm": 0.41015714406967163,
      "learning_rate": 0.00012453333333333334,
      "loss": 0.3704,
      "step": 289
    },
    {
      "epoch": 0.3841059602649007,
      "grad_norm": 0.3327294588088989,
      "learning_rate": 0.00012426666666666666,
      "loss": 0.3937,
      "step": 290
    },
    {
      "epoch": 0.38543046357615895,
      "grad_norm": 0.6054133772850037,
      "learning_rate": 0.000124,
      "loss": 0.399,
      "step": 291
    },
    {
      "epoch": 0.38675496688741723,
      "grad_norm": 0.29774609208106995,
      "learning_rate": 0.00012373333333333335,
      "loss": 0.3812,
      "step": 292
    },
    {
      "epoch": 0.3880794701986755,
      "grad_norm": 0.26012521982192993,
      "learning_rate": 0.00012346666666666667,
      "loss": 0.3514,
      "step": 293
    },
    {
      "epoch": 0.3894039735099338,
      "grad_norm": 0.25122949481010437,
      "learning_rate": 0.0001232,
      "loss": 0.3802,
      "step": 294
    },
    {
      "epoch": 0.39072847682119205,
      "grad_norm": 0.2746157646179199,
      "learning_rate": 0.00012293333333333336,
      "loss": 0.3765,
      "step": 295
    },
    {
      "epoch": 0.3920529801324503,
      "grad_norm": 0.29964056611061096,
      "learning_rate": 0.00012266666666666668,
      "loss": 0.4169,
      "step": 296
    },
    {
      "epoch": 0.3933774834437086,
      "grad_norm": 0.2822190523147583,
      "learning_rate": 0.0001224,
      "loss": 0.3866,
      "step": 297
    },
    {
      "epoch": 0.39470198675496687,
      "grad_norm": 0.34334197640419006,
      "learning_rate": 0.00012213333333333334,
      "loss": 0.3444,
      "step": 298
    },
    {
      "epoch": 0.39602649006622515,
      "grad_norm": 0.2624589502811432,
      "learning_rate": 0.00012186666666666666,
      "loss": 0.3371,
      "step": 299
    },
    {
      "epoch": 0.3973509933774834,
      "grad_norm": 0.27256524562835693,
      "learning_rate": 0.0001216,
      "loss": 0.3565,
      "step": 300
    },
    {
      "epoch": 0.39867549668874175,
      "grad_norm": 0.42620110511779785,
      "learning_rate": 0.00012133333333333335,
      "loss": 0.3582,
      "step": 301
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.2525637149810791,
      "learning_rate": 0.00012106666666666666,
      "loss": 0.3422,
      "step": 302
    },
    {
      "epoch": 0.4013245033112583,
      "grad_norm": 0.3412570357322693,
      "learning_rate": 0.0001208,
      "loss": 0.4,
      "step": 303
    },
    {
      "epoch": 0.40264900662251657,
      "grad_norm": 0.2495744526386261,
      "learning_rate": 0.00012053333333333334,
      "loss": 0.3582,
      "step": 304
    },
    {
      "epoch": 0.40397350993377484,
      "grad_norm": 0.33693093061447144,
      "learning_rate": 0.00012026666666666669,
      "loss": 0.373,
      "step": 305
    },
    {
      "epoch": 0.4052980132450331,
      "grad_norm": 0.25584250688552856,
      "learning_rate": 0.00012,
      "loss": 0.3361,
      "step": 306
    },
    {
      "epoch": 0.4066225165562914,
      "grad_norm": 0.25505882501602173,
      "learning_rate": 0.00011973333333333335,
      "loss": 0.3478,
      "step": 307
    },
    {
      "epoch": 0.40794701986754967,
      "grad_norm": 0.24354560673236847,
      "learning_rate": 0.00011946666666666668,
      "loss": 0.3515,
      "step": 308
    },
    {
      "epoch": 0.40927152317880794,
      "grad_norm": 0.2828669548034668,
      "learning_rate": 0.0001192,
      "loss": 0.3619,
      "step": 309
    },
    {
      "epoch": 0.4105960264900662,
      "grad_norm": 0.3267277479171753,
      "learning_rate": 0.00011893333333333334,
      "loss": 0.3476,
      "step": 310
    },
    {
      "epoch": 0.4119205298013245,
      "grad_norm": 0.3089945614337921,
      "learning_rate": 0.00011866666666666669,
      "loss": 0.4202,
      "step": 311
    },
    {
      "epoch": 0.41324503311258276,
      "grad_norm": 0.2839279770851135,
      "learning_rate": 0.0001184,
      "loss": 0.3674,
      "step": 312
    },
    {
      "epoch": 0.41456953642384103,
      "grad_norm": 0.2917584776878357,
      "learning_rate": 0.00011813333333333334,
      "loss": 0.3789,
      "step": 313
    },
    {
      "epoch": 0.41589403973509936,
      "grad_norm": 0.2540538012981415,
      "learning_rate": 0.00011786666666666668,
      "loss": 0.3865,
      "step": 314
    },
    {
      "epoch": 0.41721854304635764,
      "grad_norm": 0.25383010506629944,
      "learning_rate": 0.0001176,
      "loss": 0.3594,
      "step": 315
    },
    {
      "epoch": 0.4185430463576159,
      "grad_norm": 0.2825099229812622,
      "learning_rate": 0.00011733333333333334,
      "loss": 0.3858,
      "step": 316
    },
    {
      "epoch": 0.4198675496688742,
      "grad_norm": 0.306079626083374,
      "learning_rate": 0.00011706666666666668,
      "loss": 0.3665,
      "step": 317
    },
    {
      "epoch": 0.42119205298013246,
      "grad_norm": 0.264333575963974,
      "learning_rate": 0.00011679999999999999,
      "loss": 0.3337,
      "step": 318
    },
    {
      "epoch": 0.42251655629139073,
      "grad_norm": 0.25390398502349854,
      "learning_rate": 0.00011653333333333334,
      "loss": 0.3692,
      "step": 319
    },
    {
      "epoch": 0.423841059602649,
      "grad_norm": 0.2994307279586792,
      "learning_rate": 0.00011626666666666668,
      "loss": 0.3955,
      "step": 320
    },
    {
      "epoch": 0.4251655629139073,
      "grad_norm": 0.31078481674194336,
      "learning_rate": 0.000116,
      "loss": 0.3883,
      "step": 321
    },
    {
      "epoch": 0.42649006622516555,
      "grad_norm": 0.32710376381874084,
      "learning_rate": 0.00011573333333333333,
      "loss": 0.3796,
      "step": 322
    },
    {
      "epoch": 0.4278145695364238,
      "grad_norm": 0.32824939489364624,
      "learning_rate": 0.00011546666666666668,
      "loss": 0.3893,
      "step": 323
    },
    {
      "epoch": 0.4291390728476821,
      "grad_norm": 0.2603643834590912,
      "learning_rate": 0.0001152,
      "loss": 0.3656,
      "step": 324
    },
    {
      "epoch": 0.4304635761589404,
      "grad_norm": 0.27330270409584045,
      "learning_rate": 0.00011493333333333334,
      "loss": 0.3673,
      "step": 325
    },
    {
      "epoch": 0.43178807947019865,
      "grad_norm": 0.31295275688171387,
      "learning_rate": 0.00011466666666666667,
      "loss": 0.3917,
      "step": 326
    },
    {
      "epoch": 0.433112582781457,
      "grad_norm": 0.3259810507297516,
      "learning_rate": 0.0001144,
      "loss": 0.3778,
      "step": 327
    },
    {
      "epoch": 0.43443708609271525,
      "grad_norm": 0.2864232361316681,
      "learning_rate": 0.00011413333333333333,
      "loss": 0.3654,
      "step": 328
    },
    {
      "epoch": 0.4357615894039735,
      "grad_norm": 0.2782970070838928,
      "learning_rate": 0.00011386666666666668,
      "loss": 0.3748,
      "step": 329
    },
    {
      "epoch": 0.4370860927152318,
      "grad_norm": 0.38484591245651245,
      "learning_rate": 0.0001136,
      "loss": 0.3539,
      "step": 330
    },
    {
      "epoch": 0.4384105960264901,
      "grad_norm": 0.24192538857460022,
      "learning_rate": 0.00011333333333333334,
      "loss": 0.3345,
      "step": 331
    },
    {
      "epoch": 0.43973509933774835,
      "grad_norm": 0.2123347967863083,
      "learning_rate": 0.00011306666666666667,
      "loss": 0.351,
      "step": 332
    },
    {
      "epoch": 0.4410596026490066,
      "grad_norm": 0.2608535587787628,
      "learning_rate": 0.00011279999999999999,
      "loss": 0.3849,
      "step": 333
    },
    {
      "epoch": 0.4423841059602649,
      "grad_norm": 0.25138843059539795,
      "learning_rate": 0.00011253333333333334,
      "loss": 0.3457,
      "step": 334
    },
    {
      "epoch": 0.44370860927152317,
      "grad_norm": 0.30146121978759766,
      "learning_rate": 0.00011226666666666668,
      "loss": 0.381,
      "step": 335
    },
    {
      "epoch": 0.44503311258278144,
      "grad_norm": 0.6248033046722412,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.4141,
      "step": 336
    },
    {
      "epoch": 0.4463576158940397,
      "grad_norm": 0.47248008847236633,
      "learning_rate": 0.00011173333333333333,
      "loss": 0.3683,
      "step": 337
    },
    {
      "epoch": 0.447682119205298,
      "grad_norm": 0.3728291690349579,
      "learning_rate": 0.00011146666666666667,
      "loss": 0.4144,
      "step": 338
    },
    {
      "epoch": 0.44900662251655626,
      "grad_norm": 0.2214384526014328,
      "learning_rate": 0.00011120000000000002,
      "loss": 0.3545,
      "step": 339
    },
    {
      "epoch": 0.4503311258278146,
      "grad_norm": 0.2889955937862396,
      "learning_rate": 0.00011093333333333334,
      "loss": 0.3833,
      "step": 340
    },
    {
      "epoch": 0.45165562913907287,
      "grad_norm": 0.30272936820983887,
      "learning_rate": 0.00011066666666666667,
      "loss": 0.3927,
      "step": 341
    },
    {
      "epoch": 0.45298013245033114,
      "grad_norm": 0.2945750057697296,
      "learning_rate": 0.00011040000000000001,
      "loss": 0.3879,
      "step": 342
    },
    {
      "epoch": 0.4543046357615894,
      "grad_norm": 0.3655938506126404,
      "learning_rate": 0.00011013333333333333,
      "loss": 0.4472,
      "step": 343
    },
    {
      "epoch": 0.4556291390728477,
      "grad_norm": 0.2403775006532669,
      "learning_rate": 0.00010986666666666668,
      "loss": 0.371,
      "step": 344
    },
    {
      "epoch": 0.45695364238410596,
      "grad_norm": 0.24429358541965485,
      "learning_rate": 0.00010960000000000001,
      "loss": 0.3416,
      "step": 345
    },
    {
      "epoch": 0.45827814569536424,
      "grad_norm": 0.2667616903781891,
      "learning_rate": 0.00010933333333333333,
      "loss": 0.3613,
      "step": 346
    },
    {
      "epoch": 0.4596026490066225,
      "grad_norm": 0.23386630415916443,
      "learning_rate": 0.00010906666666666667,
      "loss": 0.364,
      "step": 347
    },
    {
      "epoch": 0.4609271523178808,
      "grad_norm": 0.31252098083496094,
      "learning_rate": 0.00010880000000000002,
      "loss": 0.3951,
      "step": 348
    },
    {
      "epoch": 0.46225165562913906,
      "grad_norm": 0.24611978232860565,
      "learning_rate": 0.00010853333333333333,
      "loss": 0.3655,
      "step": 349
    },
    {
      "epoch": 0.46357615894039733,
      "grad_norm": 0.25303274393081665,
      "learning_rate": 0.00010826666666666668,
      "loss": 0.3665,
      "step": 350
    },
    {
      "epoch": 0.4649006622516556,
      "grad_norm": 0.24509118497371674,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.3778,
      "step": 351
    },
    {
      "epoch": 0.46622516556291393,
      "grad_norm": 0.326625257730484,
      "learning_rate": 0.00010773333333333333,
      "loss": 0.3791,
      "step": 352
    },
    {
      "epoch": 0.4675496688741722,
      "grad_norm": 0.3572959899902344,
      "learning_rate": 0.00010746666666666667,
      "loss": 0.372,
      "step": 353
    },
    {
      "epoch": 0.4688741721854305,
      "grad_norm": 0.25835129618644714,
      "learning_rate": 0.00010720000000000002,
      "loss": 0.3642,
      "step": 354
    },
    {
      "epoch": 0.47019867549668876,
      "grad_norm": 0.23783272504806519,
      "learning_rate": 0.00010693333333333333,
      "loss": 0.3723,
      "step": 355
    },
    {
      "epoch": 0.47152317880794703,
      "grad_norm": 0.2571500539779663,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.3512,
      "step": 356
    },
    {
      "epoch": 0.4728476821192053,
      "grad_norm": 0.31059059500694275,
      "learning_rate": 0.00010640000000000001,
      "loss": 0.384,
      "step": 357
    },
    {
      "epoch": 0.4741721854304636,
      "grad_norm": 0.2589954137802124,
      "learning_rate": 0.00010613333333333333,
      "loss": 0.3573,
      "step": 358
    },
    {
      "epoch": 0.47549668874172185,
      "grad_norm": 0.2848997116088867,
      "learning_rate": 0.00010586666666666667,
      "loss": 0.3652,
      "step": 359
    },
    {
      "epoch": 0.4768211920529801,
      "grad_norm": 0.32238465547561646,
      "learning_rate": 0.0001056,
      "loss": 0.3789,
      "step": 360
    },
    {
      "epoch": 0.4781456953642384,
      "grad_norm": 0.36385616660118103,
      "learning_rate": 0.00010533333333333332,
      "loss": 0.3727,
      "step": 361
    },
    {
      "epoch": 0.4794701986754967,
      "grad_norm": 0.2914851903915405,
      "learning_rate": 0.00010506666666666667,
      "loss": 0.3754,
      "step": 362
    },
    {
      "epoch": 0.48079470198675495,
      "grad_norm": 0.2788352370262146,
      "learning_rate": 0.00010480000000000001,
      "loss": 0.3407,
      "step": 363
    },
    {
      "epoch": 0.4821192052980132,
      "grad_norm": 0.29511088132858276,
      "learning_rate": 0.00010453333333333333,
      "loss": 0.3693,
      "step": 364
    },
    {
      "epoch": 0.48344370860927155,
      "grad_norm": 0.23041221499443054,
      "learning_rate": 0.00010426666666666666,
      "loss": 0.3449,
      "step": 365
    },
    {
      "epoch": 0.4847682119205298,
      "grad_norm": 0.33898091316223145,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.3649,
      "step": 366
    },
    {
      "epoch": 0.4860927152317881,
      "grad_norm": 0.29766255617141724,
      "learning_rate": 0.00010373333333333335,
      "loss": 0.3966,
      "step": 367
    },
    {
      "epoch": 0.48741721854304637,
      "grad_norm": 0.2765381634235382,
      "learning_rate": 0.00010346666666666667,
      "loss": 0.3728,
      "step": 368
    },
    {
      "epoch": 0.48874172185430464,
      "grad_norm": 0.42740046977996826,
      "learning_rate": 0.0001032,
      "loss": 0.3597,
      "step": 369
    },
    {
      "epoch": 0.4900662251655629,
      "grad_norm": 0.30207961797714233,
      "learning_rate": 0.00010293333333333335,
      "loss": 0.3987,
      "step": 370
    },
    {
      "epoch": 0.4913907284768212,
      "grad_norm": 0.2832747995853424,
      "learning_rate": 0.00010266666666666666,
      "loss": 0.3742,
      "step": 371
    },
    {
      "epoch": 0.49271523178807947,
      "grad_norm": 0.26073670387268066,
      "learning_rate": 0.00010240000000000001,
      "loss": 0.3715,
      "step": 372
    },
    {
      "epoch": 0.49403973509933774,
      "grad_norm": 0.35905173420906067,
      "learning_rate": 0.00010213333333333335,
      "loss": 0.3654,
      "step": 373
    },
    {
      "epoch": 0.495364238410596,
      "grad_norm": 0.23046453297138214,
      "learning_rate": 0.00010186666666666667,
      "loss": 0.3237,
      "step": 374
    },
    {
      "epoch": 0.4966887417218543,
      "grad_norm": 0.26635631918907166,
      "learning_rate": 0.0001016,
      "loss": 0.3543,
      "step": 375
    },
    {
      "epoch": 0.49801324503311256,
      "grad_norm": 0.2600732147693634,
      "learning_rate": 0.00010133333333333335,
      "loss": 0.3606,
      "step": 376
    },
    {
      "epoch": 0.49933774834437084,
      "grad_norm": 0.36399292945861816,
      "learning_rate": 0.00010106666666666667,
      "loss": 0.3914,
      "step": 377
    },
    {
      "epoch": 0.5006622516556292,
      "grad_norm": 0.2806168794631958,
      "learning_rate": 0.00010080000000000001,
      "loss": 0.353,
      "step": 378
    },
    {
      "epoch": 0.5019867549668874,
      "grad_norm": 0.2550797164440155,
      "learning_rate": 0.00010053333333333334,
      "loss": 0.3829,
      "step": 379
    },
    {
      "epoch": 0.5033112582781457,
      "grad_norm": 0.2376530021429062,
      "learning_rate": 0.00010026666666666666,
      "loss": 0.3455,
      "step": 380
    },
    {
      "epoch": 0.5046357615894039,
      "grad_norm": 0.395607590675354,
      "learning_rate": 0.0001,
      "loss": 0.3728,
      "step": 381
    },
    {
      "epoch": 0.5059602649006623,
      "grad_norm": 0.3573108911514282,
      "learning_rate": 9.973333333333334e-05,
      "loss": 0.4189,
      "step": 382
    },
    {
      "epoch": 0.5072847682119205,
      "grad_norm": 0.31932100653648376,
      "learning_rate": 9.946666666666668e-05,
      "loss": 0.3966,
      "step": 383
    },
    {
      "epoch": 0.5086092715231788,
      "grad_norm": 0.2655053436756134,
      "learning_rate": 9.92e-05,
      "loss": 0.3731,
      "step": 384
    },
    {
      "epoch": 0.5099337748344371,
      "grad_norm": 0.329885870218277,
      "learning_rate": 9.893333333333333e-05,
      "loss": 0.3968,
      "step": 385
    },
    {
      "epoch": 0.5112582781456954,
      "grad_norm": 0.2668127417564392,
      "learning_rate": 9.866666666666668e-05,
      "loss": 0.3549,
      "step": 386
    },
    {
      "epoch": 0.5125827814569537,
      "grad_norm": 0.2977792024612427,
      "learning_rate": 9.84e-05,
      "loss": 0.388,
      "step": 387
    },
    {
      "epoch": 0.5139072847682119,
      "grad_norm": 0.2575432360172272,
      "learning_rate": 9.813333333333334e-05,
      "loss": 0.37,
      "step": 388
    },
    {
      "epoch": 0.5152317880794702,
      "grad_norm": 0.2492508739233017,
      "learning_rate": 9.786666666666667e-05,
      "loss": 0.3511,
      "step": 389
    },
    {
      "epoch": 0.5165562913907285,
      "grad_norm": 0.28611308336257935,
      "learning_rate": 9.76e-05,
      "loss": 0.3545,
      "step": 390
    },
    {
      "epoch": 0.5178807947019868,
      "grad_norm": 0.29152196645736694,
      "learning_rate": 9.733333333333335e-05,
      "loss": 0.3647,
      "step": 391
    },
    {
      "epoch": 0.519205298013245,
      "grad_norm": 0.2492452710866928,
      "learning_rate": 9.706666666666668e-05,
      "loss": 0.3638,
      "step": 392
    },
    {
      "epoch": 0.5205298013245033,
      "grad_norm": 0.2920679748058319,
      "learning_rate": 9.680000000000001e-05,
      "loss": 0.3265,
      "step": 393
    },
    {
      "epoch": 0.5218543046357615,
      "grad_norm": 0.2980503737926483,
      "learning_rate": 9.653333333333334e-05,
      "loss": 0.3804,
      "step": 394
    },
    {
      "epoch": 0.5231788079470199,
      "grad_norm": 0.29108068346977234,
      "learning_rate": 9.626666666666667e-05,
      "loss": 0.4073,
      "step": 395
    },
    {
      "epoch": 0.5245033112582781,
      "grad_norm": 0.2488103210926056,
      "learning_rate": 9.6e-05,
      "loss": 0.356,
      "step": 396
    },
    {
      "epoch": 0.5258278145695364,
      "grad_norm": 0.30208370089530945,
      "learning_rate": 9.573333333333335e-05,
      "loss": 0.3719,
      "step": 397
    },
    {
      "epoch": 0.5271523178807948,
      "grad_norm": 0.25554829835891724,
      "learning_rate": 9.546666666666667e-05,
      "loss": 0.3668,
      "step": 398
    },
    {
      "epoch": 0.528476821192053,
      "grad_norm": 0.2903780937194824,
      "learning_rate": 9.52e-05,
      "loss": 0.378,
      "step": 399
    },
    {
      "epoch": 0.5298013245033113,
      "grad_norm": 0.2644079625606537,
      "learning_rate": 9.493333333333334e-05,
      "loss": 0.3769,
      "step": 400
    },
    {
      "epoch": 0.5311258278145695,
      "grad_norm": 0.2477620244026184,
      "learning_rate": 9.466666666666667e-05,
      "loss": 0.3767,
      "step": 401
    },
    {
      "epoch": 0.5324503311258278,
      "grad_norm": 0.25835761427879333,
      "learning_rate": 9.44e-05,
      "loss": 0.3165,
      "step": 402
    },
    {
      "epoch": 0.5337748344370861,
      "grad_norm": 0.30245691537857056,
      "learning_rate": 9.413333333333334e-05,
      "loss": 0.3543,
      "step": 403
    },
    {
      "epoch": 0.5350993377483444,
      "grad_norm": 0.38587409257888794,
      "learning_rate": 9.386666666666667e-05,
      "loss": 0.3794,
      "step": 404
    },
    {
      "epoch": 0.5364238410596026,
      "grad_norm": 0.226238414645195,
      "learning_rate": 9.360000000000001e-05,
      "loss": 0.3225,
      "step": 405
    },
    {
      "epoch": 0.5377483443708609,
      "grad_norm": 0.23905357718467712,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.3522,
      "step": 406
    },
    {
      "epoch": 0.5390728476821192,
      "grad_norm": 0.2664177417755127,
      "learning_rate": 9.306666666666667e-05,
      "loss": 0.3675,
      "step": 407
    },
    {
      "epoch": 0.5403973509933775,
      "grad_norm": 0.2976210117340088,
      "learning_rate": 9.28e-05,
      "loss": 0.3816,
      "step": 408
    },
    {
      "epoch": 0.5417218543046357,
      "grad_norm": 0.5558599829673767,
      "learning_rate": 9.253333333333334e-05,
      "loss": 0.3987,
      "step": 409
    },
    {
      "epoch": 0.543046357615894,
      "grad_norm": 0.27500414848327637,
      "learning_rate": 9.226666666666667e-05,
      "loss": 0.372,
      "step": 410
    },
    {
      "epoch": 0.5443708609271524,
      "grad_norm": 0.25249963998794556,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.3858,
      "step": 411
    },
    {
      "epoch": 0.5456953642384106,
      "grad_norm": 0.2513192296028137,
      "learning_rate": 9.173333333333333e-05,
      "loss": 0.3587,
      "step": 412
    },
    {
      "epoch": 0.5470198675496689,
      "grad_norm": 0.3974311947822571,
      "learning_rate": 9.146666666666666e-05,
      "loss": 0.4128,
      "step": 413
    },
    {
      "epoch": 0.5483443708609271,
      "grad_norm": 0.2247873842716217,
      "learning_rate": 9.120000000000001e-05,
      "loss": 0.3303,
      "step": 414
    },
    {
      "epoch": 0.5496688741721855,
      "grad_norm": 0.21906718611717224,
      "learning_rate": 9.093333333333334e-05,
      "loss": 0.3529,
      "step": 415
    },
    {
      "epoch": 0.5509933774834437,
      "grad_norm": 0.2748141884803772,
      "learning_rate": 9.066666666666667e-05,
      "loss": 0.3598,
      "step": 416
    },
    {
      "epoch": 0.552317880794702,
      "grad_norm": 0.4029296338558197,
      "learning_rate": 9.04e-05,
      "loss": 0.3974,
      "step": 417
    },
    {
      "epoch": 0.5536423841059602,
      "grad_norm": 0.3281901478767395,
      "learning_rate": 9.013333333333333e-05,
      "loss": 0.3914,
      "step": 418
    },
    {
      "epoch": 0.5549668874172186,
      "grad_norm": 0.2889535427093506,
      "learning_rate": 8.986666666666666e-05,
      "loss": 0.3712,
      "step": 419
    },
    {
      "epoch": 0.5562913907284768,
      "grad_norm": 0.3222292959690094,
      "learning_rate": 8.960000000000001e-05,
      "loss": 0.3663,
      "step": 420
    },
    {
      "epoch": 0.5576158940397351,
      "grad_norm": 0.30715981125831604,
      "learning_rate": 8.933333333333334e-05,
      "loss": 0.3668,
      "step": 421
    },
    {
      "epoch": 0.5589403973509933,
      "grad_norm": 0.2877492606639862,
      "learning_rate": 8.906666666666667e-05,
      "loss": 0.3744,
      "step": 422
    },
    {
      "epoch": 0.5602649006622517,
      "grad_norm": 0.28304219245910645,
      "learning_rate": 8.88e-05,
      "loss": 0.3557,
      "step": 423
    },
    {
      "epoch": 0.56158940397351,
      "grad_norm": 0.2610704302787781,
      "learning_rate": 8.853333333333333e-05,
      "loss": 0.33,
      "step": 424
    },
    {
      "epoch": 0.5629139072847682,
      "grad_norm": 0.3718874752521515,
      "learning_rate": 8.826666666666668e-05,
      "loss": 0.3681,
      "step": 425
    },
    {
      "epoch": 0.5642384105960265,
      "grad_norm": 0.2855771780014038,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.3628,
      "step": 426
    },
    {
      "epoch": 0.5655629139072847,
      "grad_norm": 0.3091748058795929,
      "learning_rate": 8.773333333333333e-05,
      "loss": 0.3738,
      "step": 427
    },
    {
      "epoch": 0.5668874172185431,
      "grad_norm": 0.30412256717681885,
      "learning_rate": 8.746666666666667e-05,
      "loss": 0.3975,
      "step": 428
    },
    {
      "epoch": 0.5682119205298013,
      "grad_norm": 0.28599363565444946,
      "learning_rate": 8.72e-05,
      "loss": 0.3409,
      "step": 429
    },
    {
      "epoch": 0.5695364238410596,
      "grad_norm": 0.4215514659881592,
      "learning_rate": 8.693333333333334e-05,
      "loss": 0.3987,
      "step": 430
    },
    {
      "epoch": 0.5708609271523178,
      "grad_norm": 0.3047037422657013,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.3366,
      "step": 431
    },
    {
      "epoch": 0.5721854304635762,
      "grad_norm": 0.24202875792980194,
      "learning_rate": 8.64e-05,
      "loss": 0.321,
      "step": 432
    },
    {
      "epoch": 0.5735099337748344,
      "grad_norm": 0.39422866702079773,
      "learning_rate": 8.613333333333333e-05,
      "loss": 0.4146,
      "step": 433
    },
    {
      "epoch": 0.5748344370860927,
      "grad_norm": 0.3773503601551056,
      "learning_rate": 8.586666666666668e-05,
      "loss": 0.3457,
      "step": 434
    },
    {
      "epoch": 0.5761589403973509,
      "grad_norm": 0.3015665113925934,
      "learning_rate": 8.560000000000001e-05,
      "loss": 0.416,
      "step": 435
    },
    {
      "epoch": 0.5774834437086093,
      "grad_norm": 0.2885946035385132,
      "learning_rate": 8.533333333333334e-05,
      "loss": 0.3757,
      "step": 436
    },
    {
      "epoch": 0.5788079470198676,
      "grad_norm": 0.25277647376060486,
      "learning_rate": 8.506666666666667e-05,
      "loss": 0.3888,
      "step": 437
    },
    {
      "epoch": 0.5801324503311258,
      "grad_norm": 0.2595175802707672,
      "learning_rate": 8.48e-05,
      "loss": 0.3238,
      "step": 438
    },
    {
      "epoch": 0.5814569536423841,
      "grad_norm": 0.26020514965057373,
      "learning_rate": 8.453333333333335e-05,
      "loss": 0.3682,
      "step": 439
    },
    {
      "epoch": 0.5827814569536424,
      "grad_norm": 0.25481218099594116,
      "learning_rate": 8.426666666666668e-05,
      "loss": 0.3763,
      "step": 440
    },
    {
      "epoch": 0.5841059602649007,
      "grad_norm": 0.2628445327281952,
      "learning_rate": 8.4e-05,
      "loss": 0.3914,
      "step": 441
    },
    {
      "epoch": 0.5854304635761589,
      "grad_norm": 0.24241311848163605,
      "learning_rate": 8.373333333333334e-05,
      "loss": 0.3894,
      "step": 442
    },
    {
      "epoch": 0.5867549668874172,
      "grad_norm": 0.2677541673183441,
      "learning_rate": 8.346666666666667e-05,
      "loss": 0.3418,
      "step": 443
    },
    {
      "epoch": 0.5880794701986755,
      "grad_norm": 0.2653343677520752,
      "learning_rate": 8.32e-05,
      "loss": 0.3385,
      "step": 444
    },
    {
      "epoch": 0.5894039735099338,
      "grad_norm": 0.32055768370628357,
      "learning_rate": 8.293333333333333e-05,
      "loss": 0.3518,
      "step": 445
    },
    {
      "epoch": 0.590728476821192,
      "grad_norm": 0.2688138782978058,
      "learning_rate": 8.266666666666667e-05,
      "loss": 0.3665,
      "step": 446
    },
    {
      "epoch": 0.5920529801324503,
      "grad_norm": 0.2563084363937378,
      "learning_rate": 8.24e-05,
      "loss": 0.349,
      "step": 447
    },
    {
      "epoch": 0.5933774834437087,
      "grad_norm": 0.2558305859565735,
      "learning_rate": 8.213333333333334e-05,
      "loss": 0.3518,
      "step": 448
    },
    {
      "epoch": 0.5947019867549669,
      "grad_norm": 0.2755829095840454,
      "learning_rate": 8.186666666666667e-05,
      "loss": 0.3743,
      "step": 449
    },
    {
      "epoch": 0.5960264900662252,
      "grad_norm": 0.2577328681945801,
      "learning_rate": 8.16e-05,
      "loss": 0.316,
      "step": 450
    },
    {
      "epoch": 0.5973509933774834,
      "grad_norm": 0.32329556345939636,
      "learning_rate": 8.133333333333334e-05,
      "loss": 0.3838,
      "step": 451
    },
    {
      "epoch": 0.5986754966887418,
      "grad_norm": 0.2682136595249176,
      "learning_rate": 8.106666666666667e-05,
      "loss": 0.3464,
      "step": 452
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.2926448583602905,
      "learning_rate": 8.080000000000001e-05,
      "loss": 0.3388,
      "step": 453
    },
    {
      "epoch": 0.6013245033112583,
      "grad_norm": 0.35150474309921265,
      "learning_rate": 8.053333333333334e-05,
      "loss": 0.4085,
      "step": 454
    },
    {
      "epoch": 0.6026490066225165,
      "grad_norm": 0.28183862566947937,
      "learning_rate": 8.026666666666666e-05,
      "loss": 0.3419,
      "step": 455
    },
    {
      "epoch": 0.6039735099337749,
      "grad_norm": 0.2565954625606537,
      "learning_rate": 8e-05,
      "loss": 0.3722,
      "step": 456
    },
    {
      "epoch": 0.6052980132450331,
      "grad_norm": 0.2946309745311737,
      "learning_rate": 7.973333333333334e-05,
      "loss": 0.3804,
      "step": 457
    },
    {
      "epoch": 0.6066225165562914,
      "grad_norm": 0.26307252049446106,
      "learning_rate": 7.946666666666667e-05,
      "loss": 0.3472,
      "step": 458
    },
    {
      "epoch": 0.6079470198675496,
      "grad_norm": 0.4153575301170349,
      "learning_rate": 7.920000000000001e-05,
      "loss": 0.3557,
      "step": 459
    },
    {
      "epoch": 0.609271523178808,
      "grad_norm": 0.262941837310791,
      "learning_rate": 7.893333333333333e-05,
      "loss": 0.3249,
      "step": 460
    },
    {
      "epoch": 0.6105960264900663,
      "grad_norm": 0.6270045638084412,
      "learning_rate": 7.866666666666666e-05,
      "loss": 0.358,
      "step": 461
    },
    {
      "epoch": 0.6119205298013245,
      "grad_norm": 0.3661903142929077,
      "learning_rate": 7.840000000000001e-05,
      "loss": 0.3519,
      "step": 462
    },
    {
      "epoch": 0.6132450331125828,
      "grad_norm": 0.22720910608768463,
      "learning_rate": 7.813333333333334e-05,
      "loss": 0.3404,
      "step": 463
    },
    {
      "epoch": 0.614569536423841,
      "grad_norm": 0.27565181255340576,
      "learning_rate": 7.786666666666667e-05,
      "loss": 0.3609,
      "step": 464
    },
    {
      "epoch": 0.6158940397350994,
      "grad_norm": 0.27283984422683716,
      "learning_rate": 7.76e-05,
      "loss": 0.3366,
      "step": 465
    },
    {
      "epoch": 0.6172185430463576,
      "grad_norm": 0.3539104759693146,
      "learning_rate": 7.733333333333333e-05,
      "loss": 0.3458,
      "step": 466
    },
    {
      "epoch": 0.6185430463576159,
      "grad_norm": 0.23826688528060913,
      "learning_rate": 7.706666666666668e-05,
      "loss": 0.3327,
      "step": 467
    },
    {
      "epoch": 0.6198675496688741,
      "grad_norm": 0.3413366377353668,
      "learning_rate": 7.680000000000001e-05,
      "loss": 0.3857,
      "step": 468
    },
    {
      "epoch": 0.6211920529801325,
      "grad_norm": 0.27937737107276917,
      "learning_rate": 7.653333333333333e-05,
      "loss": 0.3678,
      "step": 469
    },
    {
      "epoch": 0.6225165562913907,
      "grad_norm": 0.28489625453948975,
      "learning_rate": 7.626666666666667e-05,
      "loss": 0.3604,
      "step": 470
    },
    {
      "epoch": 0.623841059602649,
      "grad_norm": 0.2989839017391205,
      "learning_rate": 7.6e-05,
      "loss": 0.3862,
      "step": 471
    },
    {
      "epoch": 0.6251655629139072,
      "grad_norm": 0.26929771900177,
      "learning_rate": 7.573333333333334e-05,
      "loss": 0.35,
      "step": 472
    },
    {
      "epoch": 0.6264900662251656,
      "grad_norm": 0.2859340310096741,
      "learning_rate": 7.546666666666668e-05,
      "loss": 0.3452,
      "step": 473
    },
    {
      "epoch": 0.6278145695364239,
      "grad_norm": 0.3143654465675354,
      "learning_rate": 7.52e-05,
      "loss": 0.3587,
      "step": 474
    },
    {
      "epoch": 0.6291390728476821,
      "grad_norm": 0.21746470034122467,
      "learning_rate": 7.493333333333333e-05,
      "loss": 0.3321,
      "step": 475
    },
    {
      "epoch": 0.6304635761589404,
      "grad_norm": 0.2576247453689575,
      "learning_rate": 7.466666666666667e-05,
      "loss": 0.3605,
      "step": 476
    },
    {
      "epoch": 0.6317880794701987,
      "grad_norm": 0.27845844626426697,
      "learning_rate": 7.44e-05,
      "loss": 0.3638,
      "step": 477
    },
    {
      "epoch": 0.633112582781457,
      "grad_norm": 0.2663002610206604,
      "learning_rate": 7.413333333333334e-05,
      "loss": 0.3776,
      "step": 478
    },
    {
      "epoch": 0.6344370860927152,
      "grad_norm": 0.3367784321308136,
      "learning_rate": 7.386666666666667e-05,
      "loss": 0.412,
      "step": 479
    },
    {
      "epoch": 0.6357615894039735,
      "grad_norm": 0.29864490032196045,
      "learning_rate": 7.36e-05,
      "loss": 0.3602,
      "step": 480
    },
    {
      "epoch": 0.6370860927152318,
      "grad_norm": 0.268374502658844,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.352,
      "step": 481
    },
    {
      "epoch": 0.6384105960264901,
      "grad_norm": 0.24443940818309784,
      "learning_rate": 7.306666666666668e-05,
      "loss": 0.3442,
      "step": 482
    },
    {
      "epoch": 0.6397350993377483,
      "grad_norm": 0.23421341180801392,
      "learning_rate": 7.280000000000001e-05,
      "loss": 0.3162,
      "step": 483
    },
    {
      "epoch": 0.6410596026490066,
      "grad_norm": 0.2658218741416931,
      "learning_rate": 7.253333333333334e-05,
      "loss": 0.337,
      "step": 484
    },
    {
      "epoch": 0.6423841059602649,
      "grad_norm": 0.2961198687553406,
      "learning_rate": 7.226666666666667e-05,
      "loss": 0.3741,
      "step": 485
    },
    {
      "epoch": 0.6437086092715232,
      "grad_norm": 0.2843784689903259,
      "learning_rate": 7.2e-05,
      "loss": 0.3603,
      "step": 486
    },
    {
      "epoch": 0.6450331125827815,
      "grad_norm": 0.30212196707725525,
      "learning_rate": 7.173333333333335e-05,
      "loss": 0.3439,
      "step": 487
    },
    {
      "epoch": 0.6463576158940397,
      "grad_norm": 0.2999248802661896,
      "learning_rate": 7.146666666666666e-05,
      "loss": 0.3694,
      "step": 488
    },
    {
      "epoch": 0.6476821192052981,
      "grad_norm": 0.3694503903388977,
      "learning_rate": 7.12e-05,
      "loss": 0.408,
      "step": 489
    },
    {
      "epoch": 0.6490066225165563,
      "grad_norm": 0.28490346670150757,
      "learning_rate": 7.093333333333334e-05,
      "loss": 0.367,
      "step": 490
    },
    {
      "epoch": 0.6503311258278146,
      "grad_norm": 0.28191131353378296,
      "learning_rate": 7.066666666666667e-05,
      "loss": 0.3748,
      "step": 491
    },
    {
      "epoch": 0.6516556291390728,
      "grad_norm": 0.2715533375740051,
      "learning_rate": 7.04e-05,
      "loss": 0.3663,
      "step": 492
    },
    {
      "epoch": 0.6529801324503312,
      "grad_norm": 0.24236413836479187,
      "learning_rate": 7.013333333333333e-05,
      "loss": 0.343,
      "step": 493
    },
    {
      "epoch": 0.6543046357615894,
      "grad_norm": 0.2834552824497223,
      "learning_rate": 6.986666666666667e-05,
      "loss": 0.3663,
      "step": 494
    },
    {
      "epoch": 0.6556291390728477,
      "grad_norm": 0.7683024406433105,
      "learning_rate": 6.96e-05,
      "loss": 0.3971,
      "step": 495
    },
    {
      "epoch": 0.6569536423841059,
      "grad_norm": 0.2438468188047409,
      "learning_rate": 6.933333333333334e-05,
      "loss": 0.3484,
      "step": 496
    },
    {
      "epoch": 0.6582781456953642,
      "grad_norm": 0.27735668420791626,
      "learning_rate": 6.906666666666667e-05,
      "loss": 0.401,
      "step": 497
    },
    {
      "epoch": 0.6596026490066225,
      "grad_norm": 0.23051351308822632,
      "learning_rate": 6.879999999999999e-05,
      "loss": 0.3507,
      "step": 498
    },
    {
      "epoch": 0.6609271523178808,
      "grad_norm": 0.2576099932193756,
      "learning_rate": 6.853333333333334e-05,
      "loss": 0.37,
      "step": 499
    },
    {
      "epoch": 0.6622516556291391,
      "grad_norm": 0.22539347410202026,
      "learning_rate": 6.826666666666667e-05,
      "loss": 0.338,
      "step": 500
    },
    {
      "epoch": 0.6635761589403973,
      "grad_norm": 0.2448630928993225,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.3511,
      "step": 501
    },
    {
      "epoch": 0.6649006622516557,
      "grad_norm": 0.2990151047706604,
      "learning_rate": 6.773333333333333e-05,
      "loss": 0.3677,
      "step": 502
    },
    {
      "epoch": 0.6662251655629139,
      "grad_norm": 0.25066182017326355,
      "learning_rate": 6.746666666666666e-05,
      "loss": 0.3497,
      "step": 503
    },
    {
      "epoch": 0.6675496688741722,
      "grad_norm": 0.4786977767944336,
      "learning_rate": 6.720000000000001e-05,
      "loss": 0.3577,
      "step": 504
    },
    {
      "epoch": 0.6688741721854304,
      "grad_norm": 0.265358567237854,
      "learning_rate": 6.693333333333334e-05,
      "loss": 0.3425,
      "step": 505
    },
    {
      "epoch": 0.6701986754966888,
      "grad_norm": 0.2752259075641632,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.3279,
      "step": 506
    },
    {
      "epoch": 0.671523178807947,
      "grad_norm": 0.27299317717552185,
      "learning_rate": 6.64e-05,
      "loss": 0.3426,
      "step": 507
    },
    {
      "epoch": 0.6728476821192053,
      "grad_norm": 0.28868305683135986,
      "learning_rate": 6.613333333333333e-05,
      "loss": 0.3651,
      "step": 508
    },
    {
      "epoch": 0.6741721854304635,
      "grad_norm": 0.272770494222641,
      "learning_rate": 6.586666666666666e-05,
      "loss": 0.367,
      "step": 509
    },
    {
      "epoch": 0.6754966887417219,
      "grad_norm": 0.2506672143936157,
      "learning_rate": 6.560000000000001e-05,
      "loss": 0.3171,
      "step": 510
    },
    {
      "epoch": 0.6768211920529801,
      "grad_norm": 0.23709218204021454,
      "learning_rate": 6.533333333333334e-05,
      "loss": 0.3389,
      "step": 511
    },
    {
      "epoch": 0.6781456953642384,
      "grad_norm": 0.3215690851211548,
      "learning_rate": 6.506666666666666e-05,
      "loss": 0.3966,
      "step": 512
    },
    {
      "epoch": 0.6794701986754967,
      "grad_norm": 0.28920218348503113,
      "learning_rate": 6.48e-05,
      "loss": 0.3582,
      "step": 513
    },
    {
      "epoch": 0.680794701986755,
      "grad_norm": 0.3484449088573456,
      "learning_rate": 6.453333333333333e-05,
      "loss": 0.3588,
      "step": 514
    },
    {
      "epoch": 0.6821192052980133,
      "grad_norm": 0.24839043617248535,
      "learning_rate": 6.426666666666668e-05,
      "loss": 0.3834,
      "step": 515
    },
    {
      "epoch": 0.6834437086092715,
      "grad_norm": 0.252832293510437,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.3595,
      "step": 516
    },
    {
      "epoch": 0.6847682119205298,
      "grad_norm": 0.23893512785434723,
      "learning_rate": 6.373333333333333e-05,
      "loss": 0.3548,
      "step": 517
    },
    {
      "epoch": 0.686092715231788,
      "grad_norm": 0.2513413429260254,
      "learning_rate": 6.346666666666667e-05,
      "loss": 0.3189,
      "step": 518
    },
    {
      "epoch": 0.6874172185430464,
      "grad_norm": 0.2877163887023926,
      "learning_rate": 6.32e-05,
      "loss": 0.3688,
      "step": 519
    },
    {
      "epoch": 0.6887417218543046,
      "grad_norm": 0.2737114727497101,
      "learning_rate": 6.293333333333334e-05,
      "loss": 0.3476,
      "step": 520
    },
    {
      "epoch": 0.6900662251655629,
      "grad_norm": 0.2556699216365814,
      "learning_rate": 6.266666666666667e-05,
      "loss": 0.3417,
      "step": 521
    },
    {
      "epoch": 0.6913907284768211,
      "grad_norm": 0.31860774755477905,
      "learning_rate": 6.24e-05,
      "loss": 0.3775,
      "step": 522
    },
    {
      "epoch": 0.6927152317880795,
      "grad_norm": 0.23921924829483032,
      "learning_rate": 6.213333333333333e-05,
      "loss": 0.3337,
      "step": 523
    },
    {
      "epoch": 0.6940397350993377,
      "grad_norm": 0.3496466875076294,
      "learning_rate": 6.186666666666668e-05,
      "loss": 0.3727,
      "step": 524
    },
    {
      "epoch": 0.695364238410596,
      "grad_norm": 0.2792781889438629,
      "learning_rate": 6.16e-05,
      "loss": 0.3486,
      "step": 525
    },
    {
      "epoch": 0.6966887417218544,
      "grad_norm": 0.23962315917015076,
      "learning_rate": 6.133333333333334e-05,
      "loss": 0.3554,
      "step": 526
    },
    {
      "epoch": 0.6980132450331126,
      "grad_norm": 0.24779444932937622,
      "learning_rate": 6.106666666666667e-05,
      "loss": 0.3404,
      "step": 527
    },
    {
      "epoch": 0.6993377483443709,
      "grad_norm": 0.26218360662460327,
      "learning_rate": 6.08e-05,
      "loss": 0.3493,
      "step": 528
    },
    {
      "epoch": 0.7006622516556291,
      "grad_norm": 0.43108054995536804,
      "learning_rate": 6.053333333333333e-05,
      "loss": 0.3645,
      "step": 529
    },
    {
      "epoch": 0.7019867549668874,
      "grad_norm": 0.2596096694469452,
      "learning_rate": 6.026666666666667e-05,
      "loss": 0.3665,
      "step": 530
    },
    {
      "epoch": 0.7033112582781457,
      "grad_norm": 0.25965291261672974,
      "learning_rate": 6e-05,
      "loss": 0.3448,
      "step": 531
    },
    {
      "epoch": 0.704635761589404,
      "grad_norm": 0.35444176197052,
      "learning_rate": 5.973333333333334e-05,
      "loss": 0.3745,
      "step": 532
    },
    {
      "epoch": 0.7059602649006622,
      "grad_norm": 0.2902940809726715,
      "learning_rate": 5.946666666666667e-05,
      "loss": 0.3355,
      "step": 533
    },
    {
      "epoch": 0.7072847682119205,
      "grad_norm": 0.3530271351337433,
      "learning_rate": 5.92e-05,
      "loss": 0.3668,
      "step": 534
    },
    {
      "epoch": 0.7086092715231788,
      "grad_norm": 0.2753222584724426,
      "learning_rate": 5.893333333333334e-05,
      "loss": 0.324,
      "step": 535
    },
    {
      "epoch": 0.7099337748344371,
      "grad_norm": 0.45284008979797363,
      "learning_rate": 5.866666666666667e-05,
      "loss": 0.3777,
      "step": 536
    },
    {
      "epoch": 0.7112582781456953,
      "grad_norm": 0.2673192620277405,
      "learning_rate": 5.8399999999999997e-05,
      "loss": 0.324,
      "step": 537
    },
    {
      "epoch": 0.7125827814569536,
      "grad_norm": 0.2722429037094116,
      "learning_rate": 5.813333333333334e-05,
      "loss": 0.3833,
      "step": 538
    },
    {
      "epoch": 0.713907284768212,
      "grad_norm": 0.279756635427475,
      "learning_rate": 5.7866666666666666e-05,
      "loss": 0.3519,
      "step": 539
    },
    {
      "epoch": 0.7152317880794702,
      "grad_norm": 0.24565540254116058,
      "learning_rate": 5.76e-05,
      "loss": 0.3683,
      "step": 540
    },
    {
      "epoch": 0.7165562913907285,
      "grad_norm": 0.2300015687942505,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 0.3539,
      "step": 541
    },
    {
      "epoch": 0.7178807947019867,
      "grad_norm": 0.2644272446632385,
      "learning_rate": 5.706666666666667e-05,
      "loss": 0.3003,
      "step": 542
    },
    {
      "epoch": 0.7192052980132451,
      "grad_norm": 0.2651401162147522,
      "learning_rate": 5.68e-05,
      "loss": 0.3337,
      "step": 543
    },
    {
      "epoch": 0.7205298013245033,
      "grad_norm": 0.2687031626701355,
      "learning_rate": 5.6533333333333336e-05,
      "loss": 0.3214,
      "step": 544
    },
    {
      "epoch": 0.7218543046357616,
      "grad_norm": 0.29603031277656555,
      "learning_rate": 5.626666666666667e-05,
      "loss": 0.3616,
      "step": 545
    },
    {
      "epoch": 0.7231788079470198,
      "grad_norm": 0.2652437388896942,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.3676,
      "step": 546
    },
    {
      "epoch": 0.7245033112582782,
      "grad_norm": 0.28910839557647705,
      "learning_rate": 5.573333333333334e-05,
      "loss": 0.3722,
      "step": 547
    },
    {
      "epoch": 0.7258278145695364,
      "grad_norm": 0.2654726207256317,
      "learning_rate": 5.546666666666667e-05,
      "loss": 0.3483,
      "step": 548
    },
    {
      "epoch": 0.7271523178807947,
      "grad_norm": 0.27127641439437866,
      "learning_rate": 5.520000000000001e-05,
      "loss": 0.3299,
      "step": 549
    },
    {
      "epoch": 0.7284768211920529,
      "grad_norm": 0.3831692337989807,
      "learning_rate": 5.493333333333334e-05,
      "loss": 0.3258,
      "step": 550
    },
    {
      "epoch": 0.7298013245033113,
      "grad_norm": 0.29889583587646484,
      "learning_rate": 5.466666666666666e-05,
      "loss": 0.3322,
      "step": 551
    },
    {
      "epoch": 0.7311258278145696,
      "grad_norm": 0.34799015522003174,
      "learning_rate": 5.440000000000001e-05,
      "loss": 0.3705,
      "step": 552
    },
    {
      "epoch": 0.7324503311258278,
      "grad_norm": 0.2506874203681946,
      "learning_rate": 5.413333333333334e-05,
      "loss": 0.3528,
      "step": 553
    },
    {
      "epoch": 0.7337748344370861,
      "grad_norm": 0.24518682062625885,
      "learning_rate": 5.3866666666666664e-05,
      "loss": 0.3784,
      "step": 554
    },
    {
      "epoch": 0.7350993377483444,
      "grad_norm": 0.3181171417236328,
      "learning_rate": 5.360000000000001e-05,
      "loss": 0.38,
      "step": 555
    },
    {
      "epoch": 0.7364238410596027,
      "grad_norm": 0.24434413015842438,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.3509,
      "step": 556
    },
    {
      "epoch": 0.7377483443708609,
      "grad_norm": 0.3022875189781189,
      "learning_rate": 5.3066666666666665e-05,
      "loss": 0.3879,
      "step": 557
    },
    {
      "epoch": 0.7390728476821192,
      "grad_norm": 0.25860923528671265,
      "learning_rate": 5.28e-05,
      "loss": 0.3614,
      "step": 558
    },
    {
      "epoch": 0.7403973509933774,
      "grad_norm": 0.3537028431892395,
      "learning_rate": 5.2533333333333334e-05,
      "loss": 0.3968,
      "step": 559
    },
    {
      "epoch": 0.7417218543046358,
      "grad_norm": 0.30900177359580994,
      "learning_rate": 5.2266666666666665e-05,
      "loss": 0.3555,
      "step": 560
    },
    {
      "epoch": 0.743046357615894,
      "grad_norm": 0.2625422179698944,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.3407,
      "step": 561
    },
    {
      "epoch": 0.7443708609271523,
      "grad_norm": 0.29188698530197144,
      "learning_rate": 5.1733333333333335e-05,
      "loss": 0.3953,
      "step": 562
    },
    {
      "epoch": 0.7456953642384105,
      "grad_norm": 0.31206515431404114,
      "learning_rate": 5.146666666666667e-05,
      "loss": 0.3874,
      "step": 563
    },
    {
      "epoch": 0.7470198675496689,
      "grad_norm": 0.2910672128200531,
      "learning_rate": 5.1200000000000004e-05,
      "loss": 0.3291,
      "step": 564
    },
    {
      "epoch": 0.7483443708609272,
      "grad_norm": 0.26111504435539246,
      "learning_rate": 5.0933333333333336e-05,
      "loss": 0.3519,
      "step": 565
    },
    {
      "epoch": 0.7496688741721854,
      "grad_norm": 0.2520425617694855,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 0.3288,
      "step": 566
    },
    {
      "epoch": 0.7509933774834437,
      "grad_norm": 0.30354610085487366,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 0.381,
      "step": 567
    },
    {
      "epoch": 0.752317880794702,
      "grad_norm": 0.2532656490802765,
      "learning_rate": 5.013333333333333e-05,
      "loss": 0.3017,
      "step": 568
    },
    {
      "epoch": 0.7536423841059603,
      "grad_norm": 0.32887616753578186,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.3485,
      "step": 569
    },
    {
      "epoch": 0.7549668874172185,
      "grad_norm": 0.31335920095443726,
      "learning_rate": 4.96e-05,
      "loss": 0.3188,
      "step": 570
    },
    {
      "epoch": 0.7562913907284768,
      "grad_norm": 0.28613901138305664,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.3491,
      "step": 571
    },
    {
      "epoch": 0.7576158940397351,
      "grad_norm": 0.22363686561584473,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.3526,
      "step": 572
    },
    {
      "epoch": 0.7589403973509934,
      "grad_norm": 0.2789544463157654,
      "learning_rate": 4.88e-05,
      "loss": 0.3605,
      "step": 573
    },
    {
      "epoch": 0.7602649006622516,
      "grad_norm": 0.2925412058830261,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.3108,
      "step": 574
    },
    {
      "epoch": 0.7615894039735099,
      "grad_norm": 0.2859821319580078,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.3662,
      "step": 575
    },
    {
      "epoch": 0.7629139072847683,
      "grad_norm": 0.30853351950645447,
      "learning_rate": 4.8e-05,
      "loss": 0.3565,
      "step": 576
    },
    {
      "epoch": 0.7642384105960265,
      "grad_norm": 0.35873886942863464,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.3827,
      "step": 577
    },
    {
      "epoch": 0.7655629139072848,
      "grad_norm": 0.25061920285224915,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.3374,
      "step": 578
    },
    {
      "epoch": 0.766887417218543,
      "grad_norm": 0.2640576958656311,
      "learning_rate": 4.72e-05,
      "loss": 0.3788,
      "step": 579
    },
    {
      "epoch": 0.7682119205298014,
      "grad_norm": 0.24820339679718018,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.3355,
      "step": 580
    },
    {
      "epoch": 0.7695364238410596,
      "grad_norm": 0.24986793100833893,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.3312,
      "step": 581
    },
    {
      "epoch": 0.7708609271523179,
      "grad_norm": 0.23246490955352783,
      "learning_rate": 4.64e-05,
      "loss": 0.3289,
      "step": 582
    },
    {
      "epoch": 0.7721854304635761,
      "grad_norm": 0.26044365763664246,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.3521,
      "step": 583
    },
    {
      "epoch": 0.7735099337748345,
      "grad_norm": 0.2885383367538452,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.3851,
      "step": 584
    },
    {
      "epoch": 0.7748344370860927,
      "grad_norm": 0.26780542731285095,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.3585,
      "step": 585
    },
    {
      "epoch": 0.776158940397351,
      "grad_norm": 0.26455414295196533,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.3491,
      "step": 586
    },
    {
      "epoch": 0.7774834437086092,
      "grad_norm": 0.3801955580711365,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.33,
      "step": 587
    },
    {
      "epoch": 0.7788079470198676,
      "grad_norm": 0.3956396281719208,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.3315,
      "step": 588
    },
    {
      "epoch": 0.7801324503311259,
      "grad_norm": 0.27381670475006104,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.3392,
      "step": 589
    },
    {
      "epoch": 0.7814569536423841,
      "grad_norm": 0.27074572443962097,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.3424,
      "step": 590
    },
    {
      "epoch": 0.7827814569536424,
      "grad_norm": 0.24016669392585754,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.2987,
      "step": 591
    },
    {
      "epoch": 0.7841059602649006,
      "grad_norm": 0.263994038105011,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.3535,
      "step": 592
    },
    {
      "epoch": 0.785430463576159,
      "grad_norm": 0.2756846845149994,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.3622,
      "step": 593
    },
    {
      "epoch": 0.7867549668874172,
      "grad_norm": 0.3662935793399811,
      "learning_rate": 4.32e-05,
      "loss": 0.3717,
      "step": 594
    },
    {
      "epoch": 0.7880794701986755,
      "grad_norm": 0.28276875615119934,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.3352,
      "step": 595
    },
    {
      "epoch": 0.7894039735099337,
      "grad_norm": 0.26838159561157227,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.3346,
      "step": 596
    },
    {
      "epoch": 0.7907284768211921,
      "grad_norm": 0.238493412733078,
      "learning_rate": 4.24e-05,
      "loss": 0.331,
      "step": 597
    },
    {
      "epoch": 0.7920529801324503,
      "grad_norm": 0.29637062549591064,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.3559,
      "step": 598
    },
    {
      "epoch": 0.7933774834437086,
      "grad_norm": 0.25456860661506653,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.3553,
      "step": 599
    },
    {
      "epoch": 0.7947019867549668,
      "grad_norm": 0.2403850555419922,
      "learning_rate": 4.16e-05,
      "loss": 0.336,
      "step": 600
    },
    {
      "epoch": 0.7960264900662252,
      "grad_norm": 0.26165711879730225,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.355,
      "step": 601
    },
    {
      "epoch": 0.7973509933774835,
      "grad_norm": 0.2579885721206665,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.3674,
      "step": 602
    },
    {
      "epoch": 0.7986754966887417,
      "grad_norm": 0.26274025440216064,
      "learning_rate": 4.08e-05,
      "loss": 0.3282,
      "step": 603
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.25396665930747986,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.3367,
      "step": 604
    },
    {
      "epoch": 0.8013245033112583,
      "grad_norm": 0.23201608657836914,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.3327,
      "step": 605
    },
    {
      "epoch": 0.8026490066225166,
      "grad_norm": 0.2730341851711273,
      "learning_rate": 4e-05,
      "loss": 0.3441,
      "step": 606
    },
    {
      "epoch": 0.8039735099337748,
      "grad_norm": 0.36363038420677185,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.3572,
      "step": 607
    },
    {
      "epoch": 0.8052980132450331,
      "grad_norm": 0.28254130482673645,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.3841,
      "step": 608
    },
    {
      "epoch": 0.8066225165562914,
      "grad_norm": 0.33980146050453186,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.3828,
      "step": 609
    },
    {
      "epoch": 0.8079470198675497,
      "grad_norm": 0.39648741483688354,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.3704,
      "step": 610
    },
    {
      "epoch": 0.8092715231788079,
      "grad_norm": 0.37075576186180115,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.3408,
      "step": 611
    },
    {
      "epoch": 0.8105960264900662,
      "grad_norm": 0.31620821356773376,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.3273,
      "step": 612
    },
    {
      "epoch": 0.8119205298013245,
      "grad_norm": 0.33149608969688416,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.353,
      "step": 613
    },
    {
      "epoch": 0.8132450331125828,
      "grad_norm": 0.24963970482349396,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.3029,
      "step": 614
    },
    {
      "epoch": 0.8145695364238411,
      "grad_norm": 0.2831322252750397,
      "learning_rate": 3.76e-05,
      "loss": 0.3342,
      "step": 615
    },
    {
      "epoch": 0.8158940397350993,
      "grad_norm": 0.30530601739883423,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.3744,
      "step": 616
    },
    {
      "epoch": 0.8172185430463577,
      "grad_norm": 0.24384212493896484,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.3388,
      "step": 617
    },
    {
      "epoch": 0.8185430463576159,
      "grad_norm": 0.3032602071762085,
      "learning_rate": 3.68e-05,
      "loss": 0.3591,
      "step": 618
    },
    {
      "epoch": 0.8198675496688742,
      "grad_norm": 0.23590782284736633,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.3488,
      "step": 619
    },
    {
      "epoch": 0.8211920529801324,
      "grad_norm": 0.2691725492477417,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.3669,
      "step": 620
    },
    {
      "epoch": 0.8225165562913908,
      "grad_norm": 0.24501055479049683,
      "learning_rate": 3.6e-05,
      "loss": 0.3317,
      "step": 621
    },
    {
      "epoch": 0.823841059602649,
      "grad_norm": 0.33877748250961304,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.3671,
      "step": 622
    },
    {
      "epoch": 0.8251655629139073,
      "grad_norm": 0.29858002066612244,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.376,
      "step": 623
    },
    {
      "epoch": 0.8264900662251655,
      "grad_norm": 0.25395235419273376,
      "learning_rate": 3.52e-05,
      "loss": 0.3602,
      "step": 624
    },
    {
      "epoch": 0.8278145695364238,
      "grad_norm": 0.3110128343105316,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.3617,
      "step": 625
    },
    {
      "epoch": 0.8291390728476821,
      "grad_norm": 0.24161852896213531,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.3456,
      "step": 626
    },
    {
      "epoch": 0.8304635761589404,
      "grad_norm": 0.2709059417247772,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.3519,
      "step": 627
    },
    {
      "epoch": 0.8317880794701987,
      "grad_norm": 0.24495749175548553,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.3605,
      "step": 628
    },
    {
      "epoch": 0.833112582781457,
      "grad_norm": 0.27317050099372864,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.3626,
      "step": 629
    },
    {
      "epoch": 0.8344370860927153,
      "grad_norm": 0.3049047887325287,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.3675,
      "step": 630
    },
    {
      "epoch": 0.8357615894039735,
      "grad_norm": 0.2470460683107376,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.3482,
      "step": 631
    },
    {
      "epoch": 0.8370860927152318,
      "grad_norm": 0.255346417427063,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.3558,
      "step": 632
    },
    {
      "epoch": 0.83841059602649,
      "grad_norm": 0.24195103347301483,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.3528,
      "step": 633
    },
    {
      "epoch": 0.8397350993377484,
      "grad_norm": 0.22717070579528809,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.3487,
      "step": 634
    },
    {
      "epoch": 0.8410596026490066,
      "grad_norm": 0.5571017265319824,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.3535,
      "step": 635
    },
    {
      "epoch": 0.8423841059602649,
      "grad_norm": 0.22902177274227142,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.3337,
      "step": 636
    },
    {
      "epoch": 0.8437086092715231,
      "grad_norm": 0.2690209150314331,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.326,
      "step": 637
    },
    {
      "epoch": 0.8450331125827815,
      "grad_norm": 0.24537470936775208,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.3427,
      "step": 638
    },
    {
      "epoch": 0.8463576158940397,
      "grad_norm": 0.22730737924575806,
      "learning_rate": 3.12e-05,
      "loss": 0.3462,
      "step": 639
    },
    {
      "epoch": 0.847682119205298,
      "grad_norm": 0.26027941703796387,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.3451,
      "step": 640
    },
    {
      "epoch": 0.8490066225165563,
      "grad_norm": 0.537440299987793,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.3236,
      "step": 641
    },
    {
      "epoch": 0.8503311258278146,
      "grad_norm": 0.2786860167980194,
      "learning_rate": 3.04e-05,
      "loss": 0.32,
      "step": 642
    },
    {
      "epoch": 0.8516556291390729,
      "grad_norm": 0.23699752986431122,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.3316,
      "step": 643
    },
    {
      "epoch": 0.8529801324503311,
      "grad_norm": 0.3143104612827301,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.3378,
      "step": 644
    },
    {
      "epoch": 0.8543046357615894,
      "grad_norm": 0.34170693159103394,
      "learning_rate": 2.96e-05,
      "loss": 0.3436,
      "step": 645
    },
    {
      "epoch": 0.8556291390728477,
      "grad_norm": 0.26697903871536255,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.3335,
      "step": 646
    },
    {
      "epoch": 0.856953642384106,
      "grad_norm": 0.31452223658561707,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.3675,
      "step": 647
    },
    {
      "epoch": 0.8582781456953642,
      "grad_norm": 0.24532419443130493,
      "learning_rate": 2.88e-05,
      "loss": 0.306,
      "step": 648
    },
    {
      "epoch": 0.8596026490066225,
      "grad_norm": 0.3543078601360321,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.3228,
      "step": 649
    },
    {
      "epoch": 0.8609271523178808,
      "grad_norm": 0.28249308466911316,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.3794,
      "step": 650
    },
    {
      "epoch": 0.8622516556291391,
      "grad_norm": 0.3161381483078003,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.3295,
      "step": 651
    },
    {
      "epoch": 0.8635761589403973,
      "grad_norm": 0.2393542230129242,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.3041,
      "step": 652
    },
    {
      "epoch": 0.8649006622516556,
      "grad_norm": 0.2674991488456726,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.3131,
      "step": 653
    },
    {
      "epoch": 0.866225165562914,
      "grad_norm": 0.23725298047065735,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.3645,
      "step": 654
    },
    {
      "epoch": 0.8675496688741722,
      "grad_norm": 0.37168556451797485,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.3919,
      "step": 655
    },
    {
      "epoch": 0.8688741721854305,
      "grad_norm": 0.27107787132263184,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.3509,
      "step": 656
    },
    {
      "epoch": 0.8701986754966887,
      "grad_norm": 0.285183310508728,
      "learning_rate": 2.64e-05,
      "loss": 0.35,
      "step": 657
    },
    {
      "epoch": 0.871523178807947,
      "grad_norm": 0.39691251516342163,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.3937,
      "step": 658
    },
    {
      "epoch": 0.8728476821192053,
      "grad_norm": 0.30530107021331787,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.3608,
      "step": 659
    },
    {
      "epoch": 0.8741721854304636,
      "grad_norm": 0.36693957448005676,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.3607,
      "step": 660
    },
    {
      "epoch": 0.8754966887417218,
      "grad_norm": 0.35730433464050293,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.3423,
      "step": 661
    },
    {
      "epoch": 0.8768211920529801,
      "grad_norm": 0.24303953349590302,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.3173,
      "step": 662
    },
    {
      "epoch": 0.8781456953642384,
      "grad_norm": 0.34107932448387146,
      "learning_rate": 2.48e-05,
      "loss": 0.3828,
      "step": 663
    },
    {
      "epoch": 0.8794701986754967,
      "grad_norm": 0.21300412714481354,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.3146,
      "step": 664
    },
    {
      "epoch": 0.8807947019867549,
      "grad_norm": 0.23924443125724792,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.3033,
      "step": 665
    },
    {
      "epoch": 0.8821192052980132,
      "grad_norm": 0.3127189874649048,
      "learning_rate": 2.4e-05,
      "loss": 0.3508,
      "step": 666
    },
    {
      "epoch": 0.8834437086092716,
      "grad_norm": 0.25099819898605347,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.3406,
      "step": 667
    },
    {
      "epoch": 0.8847682119205298,
      "grad_norm": 0.25596874952316284,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.3292,
      "step": 668
    },
    {
      "epoch": 0.8860927152317881,
      "grad_norm": 0.29628586769104004,
      "learning_rate": 2.32e-05,
      "loss": 0.3387,
      "step": 669
    },
    {
      "epoch": 0.8874172185430463,
      "grad_norm": 0.23705974221229553,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.3376,
      "step": 670
    },
    {
      "epoch": 0.8887417218543047,
      "grad_norm": 0.22219908237457275,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.327,
      "step": 671
    },
    {
      "epoch": 0.8900662251655629,
      "grad_norm": 0.25350913405418396,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.3595,
      "step": 672
    },
    {
      "epoch": 0.8913907284768212,
      "grad_norm": 0.22233523428440094,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.3056,
      "step": 673
    },
    {
      "epoch": 0.8927152317880794,
      "grad_norm": 0.23306836187839508,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.378,
      "step": 674
    },
    {
      "epoch": 0.8940397350993378,
      "grad_norm": 0.231703981757164,
      "learning_rate": 2.16e-05,
      "loss": 0.3385,
      "step": 675
    },
    {
      "epoch": 0.895364238410596,
      "grad_norm": 0.2270175963640213,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.3261,
      "step": 676
    },
    {
      "epoch": 0.8966887417218543,
      "grad_norm": 0.3187152147293091,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.3488,
      "step": 677
    },
    {
      "epoch": 0.8980132450331125,
      "grad_norm": 0.24049410223960876,
      "learning_rate": 2.08e-05,
      "loss": 0.3374,
      "step": 678
    },
    {
      "epoch": 0.8993377483443709,
      "grad_norm": 0.2605421841144562,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.3438,
      "step": 679
    },
    {
      "epoch": 0.9006622516556292,
      "grad_norm": 0.2873438596725464,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.3333,
      "step": 680
    },
    {
      "epoch": 0.9019867549668874,
      "grad_norm": 0.29219362139701843,
      "learning_rate": 2e-05,
      "loss": 0.3714,
      "step": 681
    },
    {
      "epoch": 0.9033112582781457,
      "grad_norm": 0.31715527176856995,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.3642,
      "step": 682
    },
    {
      "epoch": 0.904635761589404,
      "grad_norm": 0.4568919539451599,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.3813,
      "step": 683
    },
    {
      "epoch": 0.9059602649006623,
      "grad_norm": 0.24165162444114685,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.3475,
      "step": 684
    },
    {
      "epoch": 0.9072847682119205,
      "grad_norm": 0.23683953285217285,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.3228,
      "step": 685
    },
    {
      "epoch": 0.9086092715231788,
      "grad_norm": 0.2609788775444031,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.3416,
      "step": 686
    },
    {
      "epoch": 0.909933774834437,
      "grad_norm": 0.2640629708766937,
      "learning_rate": 1.84e-05,
      "loss": 0.3421,
      "step": 687
    },
    {
      "epoch": 0.9112582781456954,
      "grad_norm": 0.27618691325187683,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.3678,
      "step": 688
    },
    {
      "epoch": 0.9125827814569536,
      "grad_norm": 0.2736993730068207,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.3485,
      "step": 689
    },
    {
      "epoch": 0.9139072847682119,
      "grad_norm": 0.2609231173992157,
      "learning_rate": 1.76e-05,
      "loss": 0.3497,
      "step": 690
    },
    {
      "epoch": 0.9152317880794701,
      "grad_norm": 0.24312672019004822,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.3246,
      "step": 691
    },
    {
      "epoch": 0.9165562913907285,
      "grad_norm": 0.21823644638061523,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.3215,
      "step": 692
    },
    {
      "epoch": 0.9178807947019868,
      "grad_norm": 0.2649743854999542,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.3493,
      "step": 693
    },
    {
      "epoch": 0.919205298013245,
      "grad_norm": 0.2941141426563263,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.3838,
      "step": 694
    },
    {
      "epoch": 0.9205298013245033,
      "grad_norm": 0.27310240268707275,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.3312,
      "step": 695
    },
    {
      "epoch": 0.9218543046357616,
      "grad_norm": 0.29650580883026123,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.3697,
      "step": 696
    },
    {
      "epoch": 0.9231788079470199,
      "grad_norm": 0.46932992339134216,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.3571,
      "step": 697
    },
    {
      "epoch": 0.9245033112582781,
      "grad_norm": 0.25383460521698,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.3273,
      "step": 698
    },
    {
      "epoch": 0.9258278145695364,
      "grad_norm": 0.27388739585876465,
      "learning_rate": 1.52e-05,
      "loss": 0.3542,
      "step": 699
    },
    {
      "epoch": 0.9271523178807947,
      "grad_norm": 0.2742388844490051,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.3805,
      "step": 700
    },
    {
      "epoch": 0.928476821192053,
      "grad_norm": 0.24010281264781952,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.3677,
      "step": 701
    },
    {
      "epoch": 0.9298013245033112,
      "grad_norm": 0.34043630957603455,
      "learning_rate": 1.44e-05,
      "loss": 0.3822,
      "step": 702
    },
    {
      "epoch": 0.9311258278145695,
      "grad_norm": 0.45227089524269104,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.3656,
      "step": 703
    },
    {
      "epoch": 0.9324503311258279,
      "grad_norm": 0.23153799772262573,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.3486,
      "step": 704
    },
    {
      "epoch": 0.9337748344370861,
      "grad_norm": 0.2483360469341278,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.3482,
      "step": 705
    },
    {
      "epoch": 0.9350993377483444,
      "grad_norm": 0.2211626172065735,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.3222,
      "step": 706
    },
    {
      "epoch": 0.9364238410596026,
      "grad_norm": 0.21800366044044495,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.2875,
      "step": 707
    },
    {
      "epoch": 0.937748344370861,
      "grad_norm": 0.3133847713470459,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.3366,
      "step": 708
    },
    {
      "epoch": 0.9390728476821192,
      "grad_norm": 0.2706049382686615,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.3402,
      "step": 709
    },
    {
      "epoch": 0.9403973509933775,
      "grad_norm": 0.24645505845546722,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.3622,
      "step": 710
    },
    {
      "epoch": 0.9417218543046357,
      "grad_norm": 0.26907145977020264,
      "learning_rate": 1.2e-05,
      "loss": 0.3406,
      "step": 711
    },
    {
      "epoch": 0.9430463576158941,
      "grad_norm": 0.23111169040203094,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.3467,
      "step": 712
    },
    {
      "epoch": 0.9443708609271523,
      "grad_norm": 0.26785287261009216,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.3535,
      "step": 713
    },
    {
      "epoch": 0.9456953642384106,
      "grad_norm": 0.23209194839000702,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.3228,
      "step": 714
    },
    {
      "epoch": 0.9470198675496688,
      "grad_norm": 0.5300775170326233,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.3966,
      "step": 715
    },
    {
      "epoch": 0.9483443708609272,
      "grad_norm": 0.4437444806098938,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.3458,
      "step": 716
    },
    {
      "epoch": 0.9496688741721855,
      "grad_norm": 0.2529727816581726,
      "learning_rate": 1.04e-05,
      "loss": 0.3504,
      "step": 717
    },
    {
      "epoch": 0.9509933774834437,
      "grad_norm": 0.4392533302307129,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.3518,
      "step": 718
    },
    {
      "epoch": 0.952317880794702,
      "grad_norm": 0.2856920063495636,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.3704,
      "step": 719
    },
    {
      "epoch": 0.9536423841059603,
      "grad_norm": 0.2519843876361847,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.3608,
      "step": 720
    },
    {
      "epoch": 0.9549668874172186,
      "grad_norm": 0.24059827625751495,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.3167,
      "step": 721
    },
    {
      "epoch": 0.9562913907284768,
      "grad_norm": 0.2329767793416977,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.3361,
      "step": 722
    },
    {
      "epoch": 0.9576158940397351,
      "grad_norm": 0.2557491064071655,
      "learning_rate": 8.8e-06,
      "loss": 0.3305,
      "step": 723
    },
    {
      "epoch": 0.9589403973509933,
      "grad_norm": 0.22026285529136658,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.3182,
      "step": 724
    },
    {
      "epoch": 0.9602649006622517,
      "grad_norm": 0.28689590096473694,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.3421,
      "step": 725
    },
    {
      "epoch": 0.9615894039735099,
      "grad_norm": 0.24544551968574524,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.3471,
      "step": 726
    },
    {
      "epoch": 0.9629139072847682,
      "grad_norm": 0.24830447137355804,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.3076,
      "step": 727
    },
    {
      "epoch": 0.9642384105960264,
      "grad_norm": 0.22589968144893646,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.3232,
      "step": 728
    },
    {
      "epoch": 0.9655629139072848,
      "grad_norm": 0.32628244161605835,
      "learning_rate": 7.2e-06,
      "loss": 0.3761,
      "step": 729
    },
    {
      "epoch": 0.9668874172185431,
      "grad_norm": 0.27000609040260315,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.3317,
      "step": 730
    },
    {
      "epoch": 0.9682119205298013,
      "grad_norm": 0.24613703787326813,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.3221,
      "step": 731
    },
    {
      "epoch": 0.9695364238410596,
      "grad_norm": 0.24448347091674805,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.3376,
      "step": 732
    },
    {
      "epoch": 0.9708609271523179,
      "grad_norm": 0.3441091477870941,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.3651,
      "step": 733
    },
    {
      "epoch": 0.9721854304635762,
      "grad_norm": 0.25925424695014954,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.3608,
      "step": 734
    },
    {
      "epoch": 0.9735099337748344,
      "grad_norm": 0.28120097517967224,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.3806,
      "step": 735
    },
    {
      "epoch": 0.9748344370860927,
      "grad_norm": 0.26745569705963135,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.3371,
      "step": 736
    },
    {
      "epoch": 0.976158940397351,
      "grad_norm": 0.2691480815410614,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.3607,
      "step": 737
    },
    {
      "epoch": 0.9774834437086093,
      "grad_norm": 0.280062198638916,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.3677,
      "step": 738
    },
    {
      "epoch": 0.9788079470198675,
      "grad_norm": 0.43036162853240967,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.3338,
      "step": 739
    },
    {
      "epoch": 0.9801324503311258,
      "grad_norm": 0.3057892918586731,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.3728,
      "step": 740
    },
    {
      "epoch": 0.9814569536423841,
      "grad_norm": 0.23318475484848022,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.3377,
      "step": 741
    },
    {
      "epoch": 0.9827814569536424,
      "grad_norm": 0.22465737164020538,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.3189,
      "step": 742
    },
    {
      "epoch": 0.9841059602649007,
      "grad_norm": 0.2876679003238678,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.3616,
      "step": 743
    },
    {
      "epoch": 0.9854304635761589,
      "grad_norm": 0.2937891483306885,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.366,
      "step": 744
    },
    {
      "epoch": 0.9867549668874173,
      "grad_norm": 0.22332464158535004,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.3521,
      "step": 745
    },
    {
      "epoch": 0.9880794701986755,
      "grad_norm": 0.27802541851997375,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.3527,
      "step": 746
    },
    {
      "epoch": 0.9894039735099338,
      "grad_norm": 0.252362459897995,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.3273,
      "step": 747
    },
    {
      "epoch": 0.990728476821192,
      "grad_norm": 0.25254836678504944,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.3503,
      "step": 748
    },
    {
      "epoch": 0.9920529801324504,
      "grad_norm": 0.21519054472446442,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.342,
      "step": 749
    },
    {
      "epoch": 0.9933774834437086,
      "grad_norm": 0.27980247139930725,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.3561,
      "step": 750
    },
    {
      "epoch": 0.9947019867549669,
      "grad_norm": 0.32393506169319153,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.3554,
      "step": 751
    },
    {
      "epoch": 0.9960264900662251,
      "grad_norm": 0.27952834963798523,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.3274,
      "step": 752
    },
    {
      "epoch": 0.9973509933774835,
      "grad_norm": 0.25578486919403076,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.3532,
      "step": 753
    },
    {
      "epoch": 0.9986754966887417,
      "grad_norm": 0.30861276388168335,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.3667,
      "step": 754
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2656504809856415,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.3642,
      "step": 755
    }
  ],
  "logging_steps": 1,
  "max_steps": 755,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.239443880974746e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
