{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667a6bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb450e0-fd08-4882-9b60-353c4d1322c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING YOUR TRAINED MODEL\n",
      "================================================================================\n",
      "\n",
      "1. Loading base Llama 3.2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff31a03d3ab48dba0b4cf6faee0b4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09ca6e57ced422e993b05830beb03e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e800fa5361478690ceabefe21c4940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0c5380d01549879cb59b41549ccc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76041a6203e84e299f4866a765b2f042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a099c84a511f42cda84d038de0b0bc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb7eeb9808046d5b57b5fa03ed7f259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Base model loaded\n",
      "   ✓ LoRA adapter loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✓ Tokenizer loaded\n",
      "\n",
      "2. Analyzing your training data format...\n",
      "   Training data structure:\n",
      "     • instruction: Acting as a personalized movie recommender, analyze the user's watch history and suggest the most lo...\n",
      "     • input: User history: Bug's Life, A (1998), Antz (1998), Hunchback of Notre Dame, The (1996), Hercules (1997...\n",
      "     • output: The user should watch Pocahontas (1995). Reasoning: This movie aligns with their interest in Animati...\n",
      "\n",
      "   Using field: 'instruction' for testing\n",
      "\n",
      "3. Testing model generation...\n",
      "\n",
      "================================================================================\n",
      "TEST 1/3\n",
      "================================================================================\n",
      "PROMPT: User has watched: Toy Story (1995), Jurassic Park (1993). Recommend the next movie.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE:\n",
      "<|end_header_id|>\n",
      "\n",
      "User has watched: 101 Dalmatians (1996). Recommend the next movie.-systemusersystemusersystemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser systemuser system\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST 2/3\n",
      "================================================================================\n",
      "PROMPT: User enjoys Action and Sci-Fi movies. What should they watch next?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE:\n",
      "<|end_header_id|>\n",
      "\n",
      "user/useruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruseruser\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST 3/3\n",
      "================================================================================\n",
      "PROMPT: Based on viewing history of The Matrix, recommend similar films.\n",
      "--------------------------------------------------------------------------------\n",
      "RESPONSE:\n",
      "<|end_header_id|>\n",
      "\n",
      "• 0min - 1999\n",
      "• 98min - 1997\n",
      "• 100min - 2000\n",
      "• 90min - 1995\n",
      "• 88min - 2001\n",
      "\n",
      "Fur: An Imaginary Portrait Of Diane Arbus (2006)\n",
      "\n",
      "In the public library in New York City, an elderly woman is reading a book called Fur: The Life and Times of Diane Arbus by Patricia Bosworth. A man asks her what the movie was about. She replies that it's a biography of the famous photographer, who died young. He tells her he wants to make a movie about her. She says she doesn't know if she'd be interested.\n",
      "\n",
      "• 96min - 1984\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MODEL INFORMATION\n",
      "================================================================================\n",
      "Base model parameters: 3,237,063,680\n",
      "LoRA trainable parameters: 0\n",
      "Percentage trainable: 0.00%\n",
      "GPU memory used: 3.03 GB\n",
      "\n",
      "================================================================================\n",
      "QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "✅ Check these criteria:\n",
      "  [ ] Does the model generate movie recommendations?\n",
      "  [ ] Are recommendations relevant to the prompt?\n",
      "  [ ] Does it provide reasoning (not just list titles)?\n",
      "  [ ] Is the output coherent (not gibberish)?\n",
      "\n",
      "⚠️  If output is nonsense:\n",
      "  → Check wandb logs to see if training loss decreased\n",
      "  → Your model might need more training epochs\n",
      "\n",
      "✓ If output looks good:\n",
      "  → Proceed to creating test set and quantization\n",
      "  → Your thesis is on track!\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Test if your LoRA adapter works\n",
    "Run this to verify your model generates reasonable recommendations\n",
    "Runtime: ~2 minutes\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import json\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING YOUR TRAINED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load Base Model + Your LoRA Adapter\n",
    "# ============================================================================\n",
    "print(\"\\n1. Loading base Llama 3.2 model...\")\n",
    "\n",
    "base_model_name = \"meta-llama/Llama-3.2-3B\"  # Or wherever your base model is\n",
    "adapter_path = \"./llama3.2-lora-final\"\n",
    "\n",
    "try:\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"   ✓ Base model loaded\")\n",
    "    \n",
    "    # Load your LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    print(\"   ✓ LoRA adapter loaded\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"   ✓ Tokenizer loaded\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ✗ Error loading model: {e}\")\n",
    "    print(\"\\n⚠️  If base model not found, you need to download it first:\")\n",
    "    print(\"   huggingface-cli download meta-llama/Llama-3.2-3B\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Peek at Your Training Data Format\n",
    "# ============================================================================\n",
    "print(\"\\n2. Analyzing your training data format...\")\n",
    "\n",
    "with open('movielens_train.jsonl', 'r') as f:\n",
    "    first_sample = json.loads(f.readline())\n",
    "\n",
    "print(\"   Training data structure:\")\n",
    "for key, value in first_sample.items():\n",
    "    value_str = str(value)[:100]\n",
    "    print(f\"     • {key}: {value_str}...\")\n",
    "\n",
    "# Determine the prompt format\n",
    "if 'text' in first_sample:\n",
    "    prompt_field = 'text'\n",
    "elif 'prompt' in first_sample:\n",
    "    prompt_field = 'prompt'\n",
    "else:\n",
    "    prompt_field = list(first_sample.keys())[0]\n",
    "    \n",
    "print(f\"\\n   Using field: '{prompt_field}' for testing\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Test Generation\n",
    "# ============================================================================\n",
    "print(\"\\n3. Testing model generation...\")\n",
    "\n",
    "# Create a test prompt in the same format as your training data\n",
    "test_prompts = [\n",
    "    \"User has watched: Toy Story (1995), Jurassic Park (1993). Recommend the next movie.\",\n",
    "    \"User enjoys Action and Sci-Fi movies. What should they watch next?\",\n",
    "    \"Based on viewing history of The Matrix, recommend similar films.\"\n",
    "]\n",
    "\n",
    "for i, test_prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i}/3\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"PROMPT: {test_prompt}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Format prompt (adjust this based on your training format)\n",
    "    # Common formats:\n",
    "    # - Llama 3 format: <|begin_of_text|><|start_header_id|>user<|end_header_id|>...\n",
    "    # - Alpaca format: ### Instruction: ... ### Response:\n",
    "    \n",
    "    # Try Llama 3 format first\n",
    "    formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{test_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    # Generate\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    if \"assistant\" in full_output:\n",
    "        response = full_output.split(\"assistant\")[-1].strip()\n",
    "        response = response.replace(\"<|eot_id|>\", \"\").replace(\"<|end_of_text|>\", \"\").strip()\n",
    "    else:\n",
    "        response = full_output\n",
    "    \n",
    "    print(f\"RESPONSE:\\n{response}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Model Info\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Count parameters\n",
    "base_params = sum(p.numel() for p in base_model.parameters())\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Base model parameters: {base_params:,}\")\n",
    "print(f\"LoRA trainable parameters: {lora_params:,}\")\n",
    "print(f\"Percentage trainable: {100 * lora_params / base_params:.2f}%\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"GPU memory used: {memory_used:.2f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Quick Quality Check\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUALITY CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✅ Check these criteria:\")\n",
    "print(\"  [ ] Does the model generate movie recommendations?\")\n",
    "print(\"  [ ] Are recommendations relevant to the prompt?\")\n",
    "print(\"  [ ] Does it provide reasoning (not just list titles)?\")\n",
    "print(\"  [ ] Is the output coherent (not gibberish)?\")\n",
    "\n",
    "print(\"\\n⚠️  If output is nonsense:\")\n",
    "print(\"  → Check wandb logs to see if training loss decreased\")\n",
    "print(\"  → Your model might need more training epochs\")\n",
    "\n",
    "print(\"\\n✓ If output looks good:\")\n",
    "print(\"  → Proceed to creating test set and quantization\")\n",
    "print(\"  → Your thesis is on track!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc1acc-967b-489d-a6f0-8a506ef1c682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
