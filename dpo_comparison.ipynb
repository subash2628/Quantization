{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec747838",
   "metadata": {},
   "source": [
    "# DPO vs SFT Interactive Comparison\n",
    "## Ranking Adherence in 4-bit Quantized Movie Recommendations\n",
    "\n",
    "**Thesis**: \"Does DPO provide better ranking adherence than SFT in 4-bit quantized recommendation scenarios?\"\n",
    "\n",
    "This notebook provides interactive exploration of both models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4031ffb6",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30589b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Configuration\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(0)\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "print(f\"\\nUsing device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b80dd1",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the contrastive recommendation dataset\n",
    "dataset = load_dataset('json', data_files='contrastive_rec_train.jsonl', split='train')\n",
    "print(f\"Total dataset size: {len(dataset)} examples\")\n",
    "\n",
    "# Display sample\n",
    "sample = dataset[0]\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TRAINING EXAMPLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nInstruction:\\n{sample['instruction']}\")\n",
    "print(f\"\\nInput:\\n{sample['input']}\")\n",
    "print(f\"\\nOutput (Ground Truth):\\n{sample['output']}\")\n",
    "\n",
    "# Analyze dataset structure\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "option_a_count = sum(1 for item in dataset if \"Option A\" in item['output'])\n",
    "option_b_count = sum(1 for item in dataset if \"Option B\" in item['output'])\n",
    "\n",
    "print(f\"\\nPreference Distribution:\")\n",
    "print(f\"  Option A preferred: {option_a_count} ({option_a_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"  Option B preferred: {option_b_count} ({option_b_count/len(dataset)*100:.1f}%)\")\n",
    "print(f\"\\nPerfect for DPO: Clear preference signal with contrastive pairs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf55c3",
   "metadata": {},
   "source": [
    "## Step 2: Load Models (SFT Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"Loading SFT Baseline Model (llama3.2-lora-final)...\")\n",
    "\n",
    "try:\n",
    "    sft_model, sft_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"llama3.2-lora-final\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    print(\"✓ SFT model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not load SFT model: {e}\")\n",
    "    sft_model = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8f05d2",
   "metadata": {},
   "source": [
    "## Step 3: Load DPO Model (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3471c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading DPO Optimized Model (llama3_dpo_4bit_final)...\")\n",
    "print(\"Note: This model needs to be trained first using train_dpo.py\")\n",
    "\n",
    "try:\n",
    "    dpo_model, dpo_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"llama3_dpo_4bit_final\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    print(\"✓ DPO model loaded successfully\")\n",
    "    dpo_model = FastLanguageModel.for_inference(dpo_model)\n",
    "except Exception as e:\n",
    "    print(f\"✗ Could not load DPO model: {e}\")\n",
    "    print(\"   Run: python train_dpo.py\")\n",
    "    dpo_model = None\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f55153",
   "metadata": {},
   "source": [
    "## Step 4: Interactive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3bb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, tokenizer, sample):\n",
    "    \"\"\"\n",
    "    Run model inference on a sample and extract preference.\n",
    "    \"\"\"\n",
    "    instruction = sample['instruction']\n",
    "    input_text = sample['input']\n",
    "    \n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=False,  # For reproducibility\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response_text = full_response.split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        # Extract preference\n",
    "        if \"Option A\" in response_text:\n",
    "            preference = \"A\"\n",
    "        elif \"Option B\" in response_text:\n",
    "            preference = \"B\"\n",
    "        else:\n",
    "            preference = \"Unknown\"\n",
    "        \n",
    "        return response_text, preference\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\", \"Error\"\n",
    "\n",
    "print(\"✓ Inference function ready\")\n",
    "print(\"  Use: response, pref = run_inference(model, tokenizer, sample)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb573193",
   "metadata": {},
   "source": [
    "## Step 5: Compare Models on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample\n",
    "sample_idx = 0\n",
    "sample = dataset[sample_idx]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SAMPLE #{sample_idx}: Side-by-side Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nUser History + Movie Options:\")\n",
    "print(sample['input'])\n",
    "\n",
    "# Ground truth\n",
    "ground_truth = sample['output']\n",
    "expected_pref = \"A\" if \"Option A\" in ground_truth else \"B\"\n",
    "print(f\"\\n{'Ground Truth (Expected):':<25} {ground_truth}\")\n",
    "\n",
    "if sft_model:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"SFT Model Response:\")\n",
    "    print(\"-\"*70)\n",
    "    sft_response, sft_pref = run_inference(sft_model, sft_tokenizer, sample)\n",
    "    print(f\"Response: {sft_response}\")\n",
    "    print(f\"\\nPreference: {sft_pref} | Correct: {'✓' if sft_pref == expected_pref else '✗'}\")\n",
    "else:\n",
    "    print(\"\\nSFT model not loaded\")\n",
    "\n",
    "if dpo_model:\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"DPO Model Response:\")\n",
    "    print(\"-\"*70)\n",
    "    dpo_response, dpo_pref = run_inference(dpo_model, dpo_tokenizer, sample)\n",
    "    print(f\"Response: {dpo_response}\")\n",
    "    print(f\"\\nPreference: {dpo_pref} | Correct: {'✓' if dpo_pref == expected_pref else '✗'}\")\n",
    "else:\n",
    "    print(\"\\nDPO model not loaded. Train it first: python train_dpo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1634e1",
   "metadata": {},
   "source": [
    "## Step 6: Batch Evaluation (Optional - Small Sample)\n",
    "⚠️ **Note**: This will take a while. Recommended: Run evaluate_dpo.py instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5224db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on small subset for quick results\n",
    "test_size = 10  # Change to larger number for full evaluation\n",
    "test_samples = dataset.select(range(min(test_size, len(dataset))))\n",
    "\n",
    "results = {\n",
    "    'SFT': {'correct': 0, 'total': 0, 'preferences': []},\n",
    "    'DPO': {'correct': 0, 'total': 0, 'preferences': []}\n",
    "}\n",
    "\n",
    "print(f\"\\nEvaluating on {len(test_samples)} samples...\\n\")\n",
    "\n",
    "for idx, sample in enumerate(tqdm(test_samples, desc=\"Evaluation\")):\n",
    "    expected_pref = \"A\" if \"Option A\" in sample['output'] else \"B\"\n",
    "    \n",
    "    # SFT evaluation\n",
    "    if sft_model:\n",
    "        _, sft_pref = run_inference(sft_model, sft_tokenizer, sample)\n",
    "        results['SFT']['total'] += 1\n",
    "        if sft_pref == expected_pref:\n",
    "            results['SFT']['correct'] += 1\n",
    "        results['SFT']['preferences'].append(sft_pref == expected_pref)\n",
    "    \n",
    "    # DPO evaluation\n",
    "    if dpo_model:\n",
    "        _, dpo_pref = run_inference(dpo_model, dpo_tokenizer, sample)\n",
    "        results['DPO']['total'] += 1\n",
    "        if dpo_pref == expected_pref:\n",
    "            results['DPO']['correct'] += 1\n",
    "        results['DPO']['preferences'].append(dpo_pref == expected_pref)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if result['total'] > 0:\n",
    "        accuracy = (result['correct'] / result['total']) * 100\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  Accuracy: {accuracy:.1f}% ({result['correct']}/{result['total']})\")\n",
    "\n",
    "# Comparison\n",
    "if results['SFT']['total'] > 0 and results['DPO']['total'] > 0:\n",
    "    sft_acc = (results['SFT']['correct'] / results['SFT']['total']) * 100\n",
    "    dpo_acc = (results['DPO']['correct'] / results['DPO']['total']) * 100\n",
    "    improvement = dpo_acc - sft_acc\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DPO Improvement: {improvement:+.1f}%\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if improvement > 2:\n",
    "        print(\"✓ DPO shows meaningful improvement!\")\n",
    "    elif improvement > 0:\n",
    "        print(\"◐ DPO shows modest improvement\")\n",
    "    else:\n",
    "        print(\"✗ SFT performs better or similar to DPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ff65c",
   "metadata": {},
   "source": [
    "## Step 7: Thesis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "if results['SFT']['total'] > 0 and results['DPO']['total'] > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    sft_acc = (results['SFT']['correct'] / results['SFT']['total']) * 100\n",
    "    dpo_acc = (results['DPO']['correct'] / results['DPO']['total']) * 100\n",
    "    \n",
    "    models = ['SFT Baseline', 'DPO Optimized']\n",
    "    accuracies = [sft_acc, dpo_acc]\n",
    "    colors = ['#3498db', '#e74c3c']\n",
    "    \n",
    "    axes[0].bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    axes[0].set_ylabel('Ranking Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('DPO vs SFT: Ranking Accuracy', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylim(0, 105)\n",
    "    \n",
    "    for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
    "        axes[0].text(i, acc + 2, f'{acc:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Preference adherence over samples\n",
    "    sft_running = np.cumsum(results['SFT']['preferences']) / np.arange(1, len(results['SFT']['preferences'])+1) * 100\n",
    "    dpo_running = np.cumsum(results['DPO']['preferences']) / np.arange(1, len(results['DPO']['preferences'])+1) * 100\n",
    "    \n",
    "    axes[1].plot(sft_running, marker='o', label='SFT', linewidth=2, markersize=6, color='#3498db')\n",
    "    axes[1].plot(dpo_running, marker='s', label='DPO', linewidth=2, markersize=6, color='#e74c3c')\n",
    "    axes[1].set_xlabel('Sample Number', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Cumulative Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Preference Adherence: Convergence Curve', fontsize=13, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dpo_vs_sft_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Comparison plot saved as dpo_vs_sft_comparison.png\")\n",
    "else:\n",
    "    print(\"Cannot create visualization without both models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33738d",
   "metadata": {},
   "source": [
    "## Step 8: Thesis Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THESIS HYPOTHESIS TEST\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nQuestion: Does DPO provide better ranking adherence than SFT\")\n",
    "print(\"in 4-bit quantized recommendation scenarios?\")\n",
    "\n",
    "if results['SFT']['total'] > 0 and results['DPO']['total'] > 0:\n",
    "    sft_acc = (results['SFT']['correct'] / results['SFT']['total']) * 100\n",
    "    dpo_acc = (results['DPO']['correct'] / results['DPO']['total']) * 100\n",
    "    improvement = dpo_acc - sft_acc\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  SFT Baseline: {sft_acc:.1f}%\")\n",
    "    print(f\"  DPO Optimized: {dpo_acc:.1f}%\")\n",
    "    print(f\"  Improvement: {improvement:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\nConclusion:\")\n",
    "    if improvement > 2:\n",
    "        print(\"  ✓ HYPOTHESIS SUPPORTED\")\n",
    "        print(f\"    DPO shows {improvement:.1f}% better ranking adherence\")\n",
    "        print(\"    Preference optimization effective for 4-bit quantization\")\n",
    "    elif improvement > 0:\n",
    "        print(\"  ◐ HYPOTHESIS PARTIALLY SUPPORTED\")\n",
    "        print(f\"    DPO shows modest {improvement:.1f}% improvement\")\n",
    "        print(\"    May require larger evaluation set or hyperparameter tuning\")\n",
    "    else:\n",
    "        print(\"  ✗ HYPOTHESIS NOT SUPPORTED\")\n",
    "        print(f\"    SFT performs {abs(improvement):.1f}% better than DPO\")\n",
    "        print(\"    Consider: different beta, more training, larger model\")\n",
    "else:\n",
    "    print(\"\\nWaiting for model evaluation...\")\n",
    "    print(\"Execute previous cells to generate results\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e8a64",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Train DPO Model**\n",
    "   ```bash\n",
    "   python train_dpo.py\n",
    "   ```\n",
    "   Expected: 2-4 hours on RTX 3090\n",
    "\n",
    "2. **Run Full Evaluation**\n",
    "   ```bash\n",
    "   python evaluate_dpo.py\n",
    "   ```\n",
    "   Expected: 30-60 minutes for 100-1000 samples\n",
    "\n",
    "3. **Analyze Results**\n",
    "   - Compare accuracy metrics\n",
    "   - Check confidence scores\n",
    "   - Evaluate reasoning quality\n",
    "\n",
    "4. **Write Thesis**\n",
    "   - Summarize findings\n",
    "   - Discuss implications for quantized LLMs\n",
    "   - Propose future work"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
